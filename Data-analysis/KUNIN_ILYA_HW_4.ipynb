{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjY7F6TNVKNV"
      },
      "source": [
        "# HSE 2021: Mathematical Methods for Data Analysis\n",
        "\n",
        "## Homework 4\n",
        "\n",
        "**Warning 1**: You have 2 weeks for this assignemnt.  **it is better to start early (!)**\n",
        "\n",
        "**Warning 2**: it is critical to describe and explain what you are doing and why, use markdown cells\n",
        "\n",
        "\n",
        "### Contents\n",
        "\n",
        "#### Decision Trees - 7 points\n",
        "* [Task 1](#task1) (0.5 points)\n",
        "* [Task 2](#task2) (0.5 points)\n",
        "* [Task 3](#task3) (2 points)\n",
        "* [Task 4](#task4) (0.5 points)\n",
        "* [Task 5](#task5) (0.5 points)\n",
        "* [Task 6](#task6) (2 points)\n",
        "* [Task 7](#task7) (0.5 points)\n",
        "* [Task 8](#task8) (0.5 points)\n",
        "\n",
        "#### Ensembles - 3 points\n",
        "* [Task 1](#task2_1) (1 point)\n",
        "* [Task 2](#task2_2) (0.7 points)\n",
        "* [Task 3](#task2_3) (0.5 points)\n",
        "* [Task 4](#task2_4) (0.7 points)\n",
        "* [Task 5](#task2_5) (0.1 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "16FsiotQVKNZ"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "plt.rcParams['figure.figsize'] = (11, 5)\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHSEyuIzVKNa"
      },
      "source": [
        "# Part 1. Decision Tree Regressor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGnUANtZVKNa"
      },
      "source": [
        "In this task you will be implementing decision tree for the regression by hand. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Uw7A9P_VKNa"
      },
      "source": [
        "### Task 1 <a id=\"task1\"></a> (0.5 points)\n",
        "\n",
        "Here you should implement the function `H()` which calculates impurity criterion. We will be training regression tree, and will take mean absolute deviation as impurity criterion.\n",
        "\n",
        "* You cannot use loops\n",
        "* If `y` is empty, the function should return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yyyeRcMHVKNb"
      },
      "outputs": [],
      "source": [
        "def H(y):\n",
        "    \"\"\"\n",
        "    Calculate impurity criterion\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    y : np.array\n",
        "        array of objects target values in the node\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    H(R) : float\n",
        "        Impurity in the node (measuread by variance)\n",
        "    \"\"\"\n",
        "    if y.size == 0:\n",
        "        return 0\n",
        "    # https://www.cuemath.com/variance-formula/\n",
        "    return np.sum(np.square(y - np.sum(y) / y.size)) / y.size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ySbIcJfUVKNb"
      },
      "outputs": [],
      "source": [
        "# Test the function\n",
        "assert np.allclose(H(np.array([4, 2, 2, 2])), 0.75)\n",
        "assert np.allclose(H(np.array([])), 0.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpQieDmNVKNc"
      },
      "source": [
        "### Task 2 <a id=\"task2\"></a>  (0.5 points)\n",
        "\n",
        "To find the best split in the node we need to calculate the cost function. Denote: \n",
        "- `R` all the object in the node\n",
        "- `j` index of the feature selected for the split\n",
        "- `t` threshold\n",
        "- `R_l` and `R_r` objects in the left and right child nodes correspondingly\n",
        "\n",
        "We get the following cost function:\n",
        "\n",
        "$$\n",
        "Q(R, j, t) =\\frac{|R_\\ell|}{|R|}H(R_\\ell) + \\frac{|R_r|}{|R|}H(R_r) \\to \\min_{j, t},\n",
        "$$\n",
        "\n",
        "Implement the function `Q`, which should calculate value of the cost function for a given feature and threshold."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fiF1_UYtVKNc"
      },
      "outputs": [],
      "source": [
        "def Q(X, y, j, t):\n",
        "    \"\"\"\n",
        "    Calculate cost function\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : ndarray\n",
        "        array of objects in the node \n",
        "    y : ndarray\n",
        "        array of target values in the node \n",
        "    j : int\n",
        "        feature index (column in X)\n",
        "    t : float\n",
        "        threshold\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Q : float\n",
        "        Value of the cost function\n",
        "    \"\"\"\n",
        "    if y.size == 0:\n",
        "        return 0\n",
        "\n",
        "    # Это с семинара\n",
        "    # https://github.com/darkydash/ml_hse/blob/main/week07/07_HSE_SE_DT_solved.ipynb\n",
        "    # class DecisionTreeClassifier_from_scratch\n",
        "    # Возьмем наилучшее разделение\n",
        "    x_col = X[:, j]\n",
        "    y_left = y[x_col <= t]\n",
        "    y_right = y[x_col > t]   \n",
        "\n",
        "    # Посчитаем итоговое значение cost function\n",
        "    return partial_cost(y, y_right) + partial_cost(y, y_left)\n",
        "\n",
        "\n",
        "def partial_cost(whole, part):\n",
        "    return part.size / whole.size * H(part)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dye2l6TVVKNc"
      },
      "source": [
        "### Task 3 <a id=\"task3\"></a>  (2 points)\n",
        "\n",
        "Now, let's implement `MyDecisionTreeRegressor` class. More specifically, you need to implement the following methods:\n",
        "\n",
        "- `best_split`\n",
        "- `grow_tree`\n",
        "- `get_prediction`\n",
        "\n",
        "Also, please add `min_samples_leaf` parameter to your class\n",
        "\n",
        "Read docstrings for more details. Do not forget to use function `Q` implemented above, when finding the `best_split`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CVbY4Ud1VKNd"
      },
      "outputs": [],
      "source": [
        "class Node(object):\n",
        "    \"\"\"\n",
        "    Class for a decision tree node.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    right : Node() or None\n",
        "        Right child\n",
        "    right : Node() or None\n",
        "        Left child\n",
        "    threshold: float\n",
        "        \n",
        "    column: int\n",
        "        \n",
        "    depth: int\n",
        "        \n",
        "    prediction: float\n",
        "        prediction of the target value in the node \n",
        "        (average values calculated on a train dataset)\n",
        "    is_terminal:bool\n",
        "        indicates whether it is a terminal node (leaf) or not\n",
        "    \"\"\"    \n",
        "    def __init__(self):        \n",
        "        self.right = None\n",
        "        self.left = None\n",
        "        self.threshold = None\n",
        "        self.column = None\n",
        "        self.depth = None\n",
        "        self.is_terminal = False\n",
        "        self.prediction = None\n",
        "        \n",
        "    def __repr__(self):\n",
        "        if self.is_terminal:\n",
        "            node_desc = 'Pred: {:.2f}'.format(self.prediction)\n",
        "        else:\n",
        "            node_desc = 'Col {}, t {:.2f}, Pred: {:.2f}'. \\\n",
        "            format(self.column, self.threshold, self.prediction)\n",
        "        return node_desc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HsQ__JzlVKNd"
      },
      "outputs": [],
      "source": [
        "from sklearn.base import BaseEstimator, RegressorMixin\n",
        "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
        "\n",
        "class MyDecisionTreeRegressor(RegressorMixin, BaseEstimator):\n",
        "    \"\"\"\n",
        "    Class for a Decision Tree Regressor.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    max_depth : int\n",
        "        Max depth of a decision tree.\n",
        "    min_samples_split : int\n",
        "        Minimal number of samples (objects) in a node to make a split.\n",
        "    \"\"\"\n",
        "    def __init__(self, max_depth = 3, min_samples_split = 2, min_samples_leaf = 1):\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        # min_samples_leaf уже был добавлен, хоть в условии его просят добавить\n",
        "        self.min_samples_leaf = min_samples_leaf\n",
        "            \n",
        "    def best_split(self, X, y):\n",
        "        \"\"\"\n",
        "        Find the best split in terms of Q of data in a given decision tree node. \n",
        "        Try all features and thresholds. \n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray, shape (n_objects, n_features)\n",
        "            Objects in the parent node\n",
        "        y : ndarray, shape (n_objects, )\n",
        "            1D array with the object labels. \n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        best_split_column : int\n",
        "            Index of the best split column\n",
        "        best_threshold : float\n",
        "            The best split condition.\n",
        "        X_left : ndarray, shape (n_objects_l, n_features)\n",
        "            Objects in the left child\n",
        "        y_left : ndarray, shape (n_objects_l, )\n",
        "            Objects labels in the left child. \n",
        "        X_right : ndarray, shape (n_objects_r, n_features)\n",
        "            Objects in the right child\n",
        "        y_right : ndarray, shape (n_objects_r, )\n",
        "            Objects labels in the right child. \n",
        "        \"\"\"\n",
        "        # To store best split parameters\n",
        "        best_split_column = None\n",
        "        best_threshold = None\n",
        "        # without splitting\n",
        "        best_cost = H(y)\n",
        "        best_flag = False\n",
        "        X_left, y_left, X_right, y_right = None, None, None, None\n",
        "\n",
        "        # Цикл перебора индексов i, j\n",
        "        # На самом деле перебор параметров j, t для функции Q\n",
        "        # С целью минимизации значения данной функции (как показано в формуле)\n",
        "        for j in range(X.shape[1]):\n",
        "            for index in range(X[:,j].size):\n",
        "                current_cost = Q(X, y, j, X[index, j])\n",
        "                if current_cost < best_cost:\n",
        "                    best_split_column = j\n",
        "                    best_threshold = X[index, j]\n",
        "                    best_cost = current_cost\n",
        "                    best_flag = True\n",
        "\n",
        "        if best_flag:\n",
        "            # Это с семинара\n",
        "            # https://github.com/darkydash/ml_hse/blob/main/week07/07_HSE_SE_DT_solved.ipynb\n",
        "            # class DecisionTreeClassifier_from_scratch\n",
        "            x_col = X[:, best_split_column]\n",
        "            X_left = X[x_col <= best_threshold, :]\n",
        "            y_left = y[x_col <= best_threshold]\n",
        "            X_right = X[x_col > best_threshold, :]\n",
        "            y_right = y[x_col > best_threshold]\n",
        "\n",
        "        return best_split_column, best_threshold, X_left, y_left, X_right, y_right\n",
        "    \n",
        "    def is_terminal(self, node, y):\n",
        "        \"\"\"\n",
        "        Check terminality conditions based on `max_depth`, \n",
        "        `min_samples_split` and `min_samples_leaf` parameters for a given node. \n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        node : Node, \n",
        "\n",
        "        y : ndarray, shape (n_objects, )\n",
        "            Object labels. \n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        Is_termial : bool\n",
        "            If True, node is terminal\n",
        "        \"\"\"\n",
        "        # Поменял на нестрогое, так как счетчик начинается с 1\n",
        "        if node.depth > self.max_depth:\n",
        "            return True\n",
        "        if len(y) < self.min_samples_split:\n",
        "            return True\n",
        "        return False\n",
        "        \n",
        "    def grow_tree(self, node, X, y):\n",
        "        \"\"\"\n",
        "        Reccurently grow the tree from the `node` using a `X` and `y` as a dataset:\n",
        "         - check terminality conditions\n",
        "         - find best split if node is not terminal\n",
        "         - add child nodes to the node\n",
        "         - call the function recursively for the added child nodes\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        node : Node() object\n",
        "            Current node of the decision tree.\n",
        "        X : ndarray, shape (n_objects, n_features)\n",
        "            Objects \n",
        "        y : ndarray, shape (n_objects)\n",
        "            Labels\n",
        "        \"\"\"\n",
        "        if self.is_terminal(node, y):\n",
        "            node.is_terminal = True\n",
        "            return\n",
        "        \n",
        "        # Общая логика метода взята с семинара\n",
        "        # https://github.com/darkydash/ml_hse/blob/main/week07/07_HSE_SE_DT_solved.ipynb\n",
        "        # Клетка 10, def decision_tree\n",
        "        if X.size < self.min_samples_split:   # min_samples_split check\n",
        "            node.is_terminal = True\n",
        "            return\n",
        "\n",
        "        # Вызов best_split, написанного ранее, для получения наилучшего разделения (с минимальной \"ценой\")\n",
        "        best_split_column, best_threshold, X_left, y_left, X_right, y_right = self.best_split(X, y)\n",
        "\n",
        "        # Check additional termination conditions\n",
        "        # https://github.com/darkydash/ml_hse/blob/main/week07/07_HSE_SE_DT_solved.ipynb\n",
        "        if X_right is None:\n",
        "            node.is_terminal = True\n",
        "            return\n",
        "        if X_right.size < self.min_samples_leaf or X_left.size < self.min_samples_leaf:  # min_samples_leaf check\n",
        "            node.is_terminal = True\n",
        "            return\n",
        "\n",
        "        # \"Проростание\" двух вершин из текущей\n",
        "        node.right, node.left = Node(), Node()\n",
        "\n",
        "        # Заполнение оставшихся значений для текущей вершины\n",
        "        node.column = best_split_column\n",
        "        node.threshold = best_threshold\n",
        "\n",
        "        # Заполнение части значений для новых вершин\n",
        "        self.fill_node(node.right, node.depth, y_right)\n",
        "        self.fill_node(node.left, node.depth, y_left)\n",
        "\n",
        "        # Рекурсивный рост дерева\n",
        "        self.grow_tree(node.right, X_right, y_right)\n",
        "        self.grow_tree(node.left, X_left, y_left)\n",
        "\n",
        "    # Сделано по аналогии метода fit()\n",
        "    # Заполнение части значений для новых вершин\n",
        "    def fill_node(self, partial_node, depth, previous):\n",
        "        partial_node.depth = depth + 1\n",
        "        partial_node.prediction = np.mean(previous)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Fit the Decision Tree Regressor.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray, shape (n_samples, n_features)\n",
        "            The input samples.\n",
        "        y : ndarray, shape (n_samples,) or (n_samples, n_outputs)\n",
        "            The target values.\n",
        "        Returns\n",
        "        -------\n",
        "        self : object\n",
        "            Returns self.\n",
        "        \"\"\"\n",
        "        X, y = check_X_y(X, y, accept_sparse = False)\n",
        "        self.is_fitted_ = True\n",
        "\n",
        "        # Initialize the tree (root node)\n",
        "        self.tree_ = Node()\n",
        "        self.tree_.depth = 1\n",
        "        self.tree_.prediction = np.mean(y)\n",
        "\n",
        "        # Grow the tree\n",
        "        self.grow_tree(self.tree_, X, y)\n",
        "        return self       \n",
        "    \n",
        "    def get_prediction(self, node, x):\n",
        "        \"\"\"\n",
        "        Get prediction for an object `x`\n",
        "            - Return prediction of the `node` if it is terminal\n",
        "            - Otherwise, recursively call the function to get \n",
        "            predictions of the proper child\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        node : Node() object\n",
        "            Current node of the decision tree.\n",
        "        x : ndarray, shape (n_features,)\n",
        "            Array of feature values of one object.\n",
        "        Returns\n",
        "        -------\n",
        "        y_pred : float\n",
        "            Prediction for an object x\n",
        "        \"\"\"\n",
        "        # Конец вычисления\n",
        "        if node.is_terminal:   \n",
        "            return node.prediction              \n",
        "\n",
        "        # Выбор оптимальной вершины (левой или правой)\n",
        "        optimal_node = node.left\n",
        "        if x[node.column] > node.threshold:\n",
        "            optimal_node = node.right\n",
        "\n",
        "        # Рекурсивное вычисление (шагаем до листа)\n",
        "        return self.get_prediction(optimal_node, x)\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\" \n",
        "        Get prediction for each object in X\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray, shape (n_samples, n_features)\n",
        "            The input samples.\n",
        "        Returns\n",
        "        -------\n",
        "        y : ndarray, shape (n_samples,)\n",
        "            Returns predictions.\n",
        "        \"\"\"\n",
        "        # Check input and that `fit` had been called\n",
        "        X = check_array(X, accept_sparse = False)\n",
        "        check_is_fitted(self, 'is_fitted_')\n",
        "\n",
        "        # Get predictions\n",
        "        y_predicted = []\n",
        "        for x in X:\n",
        "            y_curr = self.get_prediction(self.tree_, x)\n",
        "            y_predicted.append(y_curr)\n",
        "        return np.array(y_predicted)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bq7GsmluVKNe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e0f4df6-1da7-4693-8885-b3f1da8fb8e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/estimator_checks.py:3617: FutureWarning: As of scikit-learn 0.23, estimators should expose a n_features_in_ attribute, unless the 'no_validation' tag is True. This attribute should be equal to the number of features passed to the fit method. An error will be raised from version 1.0 (renaming of 0.25) when calling check_estimator(). See SLEP010: https://scikit-learn-enhancement-proposals.readthedocs.io/en/latest/slep010/proposal.html\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/estimator_checks.py:3652: FutureWarning: As of scikit-learn 0.23, estimators should have a 'requires_y' tag set to the appropriate value. The default value of the tag is False. An error will be raised from version 1.0 when calling check_estimator() if the tag isn't properly set.\n",
            "  warnings.warn(warning_msg, FutureWarning)\n"
          ]
        }
      ],
      "source": [
        "# check yourself\n",
        "from sklearn.utils.estimator_checks import check_estimator\n",
        "\n",
        "check_estimator(MyDecisionTreeRegressor())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Появилось два warning, но я так понимаю не подразумевается, что мы будем их исправлять. Раз ячейка отработала, значик все ок.\n",
        "\n",
        "По поводу описания написанного кода: каждое действие прокомментировал в самом коде (методы best_split(), grow_tree(), get_prediction()).\n",
        "\n",
        "Best_split - ищет разбиение с наименьшой \"ценой\".\n",
        "\n",
        "Grow_tree (рекурсивная) - растит дерево, как раз используя best_split.\n",
        "\n",
        "Get_prediction (рекурсивная) - пробегают по дереву (не по всему, а как бы по высоте дерева) и приходят в лист, из которого берется prediction."
      ],
      "metadata": {
        "id": "nRd5jkAU8ra6"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUU6KwZhVKNf"
      },
      "source": [
        "### Task 4 <a id=\"task4\"></a>  (0.5 points)\n",
        "\n",
        "Load boston dataset and split it on the train ($75\\%$) and test ($25\\%$). Fit Decision Tree of depth 1 and make the following plot:\n",
        "\n",
        "- Scatter plot of the traning points (selected for split feature on the x-axis, target variable on the y-axis)\n",
        "- Fitted model "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M2VGN_amVKNg"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_boston\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# Клетка 25\n",
        "# https://github.com/darkydash/ml_hse/blob/main/week07/07_HSE_SE_DT_solved.ipynb\n",
        "def plot_graphic(values_x_1, values_y_1, values_x_2, values_y_2):\n",
        "    plt.figure()\n",
        "    plt.scatter(values_x_1, values_y_1, label = 'Train')      \n",
        "    plt.scatter(values_x_2, values_y_2, label = 'Prediction')\n",
        "    plt.xlabel('Split feature') # set up X-axis label\n",
        "    plt.ylabel('Target variable') # set up Y-axis label\n",
        "    plt.legend(loc='best') # create the plot legend and set up it position\n",
        "    plt.grid(b=1) # create grid on the plot\n",
        "    plt.show() # display the plot\n",
        "\n",
        "# Загрузка данных\n",
        "data = load_boston()\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size = 0.25, random_state = 18293)\n",
        "\n",
        "# Обучение дерева\n",
        "tree = MyDecisionTreeRegressor(max_depth = 1)\n",
        "tree.fit(X_train, y_train)\n",
        "prediction = tree.predict(X_test)\n",
        "\n",
        "# Рисование графики\n",
        "plot_graphic(X_train[:, tree.tree_.column], y_train, X_test[:, tree.tree_.column], prediction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_F-VZCU6VKNg"
      },
      "source": [
        "### Task 5 <a id=\"task5\"></a>  (0.5 points)\n",
        "\n",
        "Keep working with boston dataset. \n",
        "- Use `GridSearchCV` to find the best hyperparameters among [`max_depth`, `min_samples_leaf`] on 5-Fold cross-validation\n",
        "- Train the model with the best set of hyperparameters on the whole train dataset. \n",
        "- Report `MAE` on test dataset and hyperparameters of the best estimator. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7pClVtp4VKNg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a5cc512-5f66-4484-c68a-554b1f114385"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAE = 3.1734786168970253\n",
            "Лучшие параметры = {'max_depth': 15, 'min_samples_leaf': 15}\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# Ввод values, по которым будет поиск\n",
        "max_depth_values = [1, 5, 10, 15, 20, 25, 30]\n",
        "min_samples_leaf_values = [1, 5, 10, 15, 20, 25, 30, 35, 40]\n",
        "\n",
        "# Поиск наилучших параметров по метрике neg_mean_squared_error\n",
        "# При выборе метрики опирался на семинар\n",
        "# Там использованы accuracy и neg_mean_squared_error\n",
        "# Но при использовнии accuracy здесь, на каждой итерации выдается warning\n",
        "searcher = GridSearchCV(MyDecisionTreeRegressor(), {\"max_depth\": max_depth_values, \"min_samples_leaf\" : min_samples_leaf_values}, scoring = \"neg_mean_squared_error\", cv = 5)\n",
        "searcher.fit(X_train, y_train)\n",
        "best_param = searcher.best_params_\n",
        "\n",
        "# Обучение\n",
        "tree = MyDecisionTreeRegressor(max_depth = best_param[\"max_depth\"], min_samples_leaf = best_param[\"min_samples_leaf\"])\n",
        "tree.fit(X_train, y_train)\n",
        "prediction = tree.predict(X_test)\n",
        "\n",
        "# Вывод полученных значений\n",
        "MAE = mean_absolute_error(y_test, prediction)\n",
        "print(f\"MAE = {MAE}\")\n",
        "print(f\"Лучшие параметры = {best_param}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPn3-CwLVKNh"
      },
      "source": [
        "### Task 6 <a id=\"task6\"></a>  (2 points)\n",
        "\n",
        "Recall definition of bias and variance:\n",
        "$$\n",
        "\\text{Bias}^2 = \\mathbb{E}_{p(x, y)} \\left[  (f(x) - \\mathbb{E}_{\\mathbb{X}}a_{\\mathbb{X}}(x))^2 \\right] \\\\\n",
        "\\text{Variance} = \\mathbb{E}_{p(x, y)} \\left[  \\mathbb{V}_{\\mathbb{X}}( a_{\\mathbb{X}}(x))  \\right]\n",
        "$$\n",
        "\n",
        "We wil now use use the following algorithm to estimate bias and variance:\n",
        "\n",
        "1. Use bootsrap to create `n_iter` samples from the original dataset: $X_1, \\dots, X_{n_iter}$\n",
        "2. For each bootstrapped sample define out-of-bag (OOB) sample $Z_1, \\dots, Z_{n_iter}$, which contain all the observations, which did not appear in the corresponding boostraped sample\n",
        "3. Fit the model on $X_i$s and compute predictions on $Z_i$s\n",
        "4. For a given *object* $n$:\n",
        "     - bias^2: squared difference between true value $y_n$ and average prediction (average over the algorithms, for which $n$ was in OOB)\n",
        "     - variance: variance of the prediction (predictions of the algorithms, for which $n$ was in OOB)\n",
        "5. Average bias^2 and variance over all the points\n",
        "    \n",
        "**Implement `get_bias_variance` function, using the algorithm above**\n",
        "\n",
        "*Note:*  You can only use 1 loop (for bootsrap iterations). All other operations should be vectorized. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7esBbVuCVKNh"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "def get_bias_variance(estimator, x, y, n_iter):\n",
        "    \"\"\" \n",
        "    Calculate bias and variance of the `estimator`. \n",
        "    Using a given dataset and bootstrap with `n_iter` samples. \n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    x : ndarray, shape (n_samples, n_features)\n",
        "        The input samples.\n",
        "    y : ndarray, shape (n_samples, n_features)\n",
        "        The input samples.\n",
        "    n_iter: int\n",
        "        Number of samples in \n",
        "    Returns\n",
        "    -------\n",
        "    bias2 : float, \n",
        "        Estiamted squared bias\n",
        "    variance : float, \n",
        "        Estiamted variance\n",
        "    \"\"\"\n",
        "    random.seed(1723987) # сид рандома\n",
        "    all_idx = np.array(range(y.size)) # просто все индексы y\n",
        "    bias = np.zeros(n_iter) # массив, куда будем сохранять bias каждой итерации\n",
        "    variance = np.zeros(n_iter) # массив, куда будем сохранять variance каждой итерации\n",
        "    \n",
        "    # https://medium.com/swlh/bootstrap-sampling-using-pythons-numpy-85822d868977\n",
        "    for i in range(n_iter):\n",
        "        # 1. Create n_iter samples from the original dataset\n",
        "        # https://docs-python.ru/standart-library/modul-random-python/funktsija-random-choices/\n",
        "        boot = random.choices(all_idx, k = all_idx.size)\n",
        "        # Аналог разделения по threshold с того же семинара\n",
        "        X_train_local, y_train_local = x[boot, :], y[boot]\n",
        "\n",
        "        # 2. For each bootstrapped sample define out-of-bag (OOB) sample\n",
        "        # https://stackoverflow.com/questions/55573872/how-to-subset-numpy-array-with-exclusion\n",
        "        oob = all_idx[np.isin(all_idx, boot, invert = True)]\n",
        "        X_test_local, y_test_local = x[oob, :], y[oob]\n",
        "\n",
        "        # 3. Fit the model and compute predictions\n",
        "        estimator.fit(X_train_local, y_train_local)\n",
        "        pred_local = estimator.predict(X_test_local)\n",
        "\n",
        "        # 4. Calculate and save bias^2 and variance\n",
        "        # https://www.cuemath.com/variance-formula/\n",
        "        bias[i] = np.mean(np.square(y_test_local - np.mean(pred_local)))\n",
        "        variance[i] = np.sum(np.square(pred_local - np.sum(pred_local) / pred_local.size)) / pred_local.size\n",
        "    \n",
        "    # 5. Average bias^2 and variance over all the points\n",
        "    return np.mean(bias), np.mean(variance)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IaQFtSAhVKNi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8cfb1f2-f168-4e54-8357-3f4498129a0e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(83.24794232096762, 82.35336205387762)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# Test\n",
        "estimator = MyDecisionTreeRegressor(max_depth=8, min_samples_split=15)\n",
        "\n",
        "# get_bias_variance(estimator, X_train, y_train, 10)\n",
        "get_bias_variance(estimator, X_train, y_train, 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsI0NlukVKNi"
      },
      "source": [
        "### Task 7 <a id=\"task7\"></a>  (0.5 points)\n",
        "\n",
        "Compute bias and variance for the trees with different min_samples_split. Plot how bias and variance change as min_samples_split increases. \n",
        "\n",
        "Comment on what you observe, how does your result correspond to theory?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aTJstPe3VKNk"
      },
      "outputs": [],
      "source": [
        "# Массивы для bias, variance\n",
        "biases, variances = [], []\n",
        "\n",
        "# Просчет bias, variance для каждого значения min_samples_split\n",
        "min_samples_split = [1, 3, 5, 7, 10, 13, 15, 17, 20]\n",
        "for min_sample_split in min_samples_split:\n",
        "    estimator = MyDecisionTreeRegressor(max_depth = 10, min_samples_split = min_sample_split)\n",
        "    bias, variance = get_bias_variance(estimator, X_train, y_train, 10)\n",
        "    # Сохранения bias, variance\n",
        "    biases.append(bias)\n",
        "    variances.append(variance)\n",
        "\n",
        "# Рисования графика\n",
        "plt.plot(min_samples_split, biases, label = \"Biases\")\n",
        "plt.plot(min_samples_split, variances, label = \"Variances\")\n",
        "plt.xlabel(\"Min samples split\")\n",
        "plt.ylabel(\"Bias / variance\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LoIzd27rVKNk"
      },
      "source": [
        "Как мы знаем, bias - метрика, считающая системную ошибку предположения в процессе машинного обучения. Variance - это изменчивость формы целевой функции относительно различных обучающих наборов. Модели с высокой дисперсией могут пострадать даже при небольших изменениях в обучающем наборе. DecisionTreeRegressor очень чувствителен к различиям в данных.\n",
        "\n",
        "Не смотря на то, что один из графиков визуально прыгает, на самом деле оба графики практически горизонтальны, ведь все значения находятся в диапазоне от 82 до 84. В данном случае min_sample_split практически не влияет на bias / variance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJBzDx3hVKNk"
      },
      "source": [
        "### Task 8 <a id=\"task8\"></a>  (0.5 points)\n",
        "\n",
        "Let's try to reduce variance with bagging. Use `sklearn.ensemble.BaggingRegressor` to get an ensemble and compute its bias and variance. \n",
        "\n",
        "Answer the following questions:\n",
        " - How bagging should affect bias and variance in theory?\n",
        " - How bias and variance change (if they change) compared to an individual tree in you experiments? \n",
        " - Do your results align with the theory? Why?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BmBuI-VZVKNl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7475267-ef02-4af5-cc10-8a64af8f3723"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bias = 83.03033198426117\n",
            "Variance = 71.00277606100961\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import BaggingRegressor\n",
        "\n",
        "regressor = BaggingRegressor(MyDecisionTreeRegressor(max_depth = 10, min_samples_split = 10), random_state = 1723987)\n",
        "bias_bagging, variance_bagging = get_bias_variance(regressor, X_train, y_train, 10)\n",
        "print(f\"Bias = {bias_bagging}\")\n",
        "print(f\"Variance = {variance_bagging}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How bagging should affect bias and variance in theory?**\n",
        "\n",
        " Не затронуть bias\n",
        "\n",
        " Уменьшить variance"
      ],
      "metadata": {
        "id": "2U0qStahlpgz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How bias and variance change (if they change) compared to an individual tree in you experiments?**\n",
        "\n",
        "Bias не был затронут\n",
        "\n",
        "Variance уменьшился"
      ],
      "metadata": {
        "id": "8B_To8FNmFn-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Do your results align with the theory? Why?**\n",
        "\n",
        "Да, в точности сошлись с теорией\n"
      ],
      "metadata": {
        "id": "oJLUcDfqmHU3"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}