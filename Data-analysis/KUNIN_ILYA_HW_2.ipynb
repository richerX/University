{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJ7RSbXIwYJG"
      },
      "source": [
        "# HSE 2021: Mathematical Methods for Data Analysis\n",
        "\n",
        "## Homework 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-26T16:48:20.566549Z",
          "start_time": "2020-09-26T16:48:19.893995Z"
        },
        "id": "OQZp_BG5wYJI"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import sklearn\n",
        "from sklearn import datasets\n",
        "from sklearn.datasets import load_boston\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.regression.linear_model import OLSResults\n",
        "from math import sqrt\n",
        "import random\n",
        "import sys\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "sns.set(style=\"darkgrid\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "It03k-PMwYJK"
      },
      "source": [
        "### Data\n",
        "\n",
        "For this homework we use Dataset from seaborn on diamonds prices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IftPrT8GwYJK"
      },
      "outputs": [],
      "source": [
        "data = sns.load_dataset('diamonds')\n",
        "\n",
        "y = data.price\n",
        "X = data.drop(['price'], axis=1)\n",
        "columns = data.drop(['price'], axis=1).columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PViagWIawYJK"
      },
      "source": [
        "## Linear regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKn1qwr_wYJL"
      },
      "source": [
        "#### 0. [0.25 points] Encode categorical variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JyRpm3ajwYJL"
      },
      "outputs": [],
      "source": [
        "# your code here\n",
        "to_encode = [\"cut\", \"color\", \"clarity\"]\n",
        "\n",
        "encoding = dict()\n",
        "for word in to_encode:\n",
        "    encoding[word] = dict()\n",
        "    for index, elem in enumerate(data[word].unique()):\n",
        "        encoding[word][elem] = index\n",
        "X = X.replace(encoding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1II9g5ewYJL"
      },
      "source": [
        "#### 1. [0.25 points] Split the data into train and test sets with ratio 80:20 with random_state=17."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PyRw8ZuHwYJL"
      },
      "outputs": [],
      "source": [
        "# your code here \n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=17)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCowstFwwYJM"
      },
      "source": [
        "#### 2. [1 point] Train models on train data using StatsModels library and apply it to the test set; use $RMSE$ and $R^2$ as the quality measure.\n",
        "\n",
        "* [`LinearRegression`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html);\n",
        "* [`Ridge`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) with $\\alpha = 0.01$;\n",
        "* [`Lasso`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html) with $\\alpha = 0.01$\n",
        "* [`ElasticNet`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html) with $\\alpha = 0.01$, $l_{1}$_$ratio = 0.6$\n",
        "\n",
        "Don't forget to scale the data before training the models with StandardScaler!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eYbikzm_wYJM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56903930-9a71-40d8-a7e5-6108746a5124"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Linear] RMSE = 1448.264793807763\n",
            "[Linear] R2   = 0.8704754690827957\n",
            "\n",
            "[Ridge] RMSE = 1456.7092504515917\n",
            "[Ridge] R2   = 0.8689606176297138\n",
            "\n",
            "[Lasso] RMSE = 1446.990447222547\n",
            "[Lasso] R2   = 0.8707033093583628\n",
            "\n",
            "[ElasticNet] RMSE = 1448.229656468968\n",
            "[ElasticNet] R2   = 0.8704817539723205\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# your code here \n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def print_quality(y_test, prediction, type_str):\n",
        "    print(f'[{type_str}] RMSE = {mean_squared_error(y_test, prediction, squared = False)}')\n",
        "    print(f'[{type_str}] R2   = {r2_score(y_test, prediction)}')\n",
        "    print()\n",
        "\n",
        "def get_fit(model, type_str):\n",
        "    if type_str == \"Linear\":\n",
        "        return model.fit()\n",
        "    elif type_str == \"Ridge\":\n",
        "        return model.fit_regularized(alpha = 0.01, L1_wt = 2 * sys.float_info.epsilon)\n",
        "    elif type_str == \"Lasso\":\n",
        "        return model.fit_regularized(alpha = 0.01, L1_wt = 1)\n",
        "    elif type_str == \"ElasticNet\":\n",
        "        return model.fit_regularized(alpha = 0.01, L1_wt = 0.6)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = sm.add_constant(scaler.fit_transform(X_train))\n",
        "X_test_scaled = sm.add_constant(scaler.transform(X_test))\n",
        "\n",
        "types = [\"Linear\", \"Ridge\", \"Lasso\", \"ElasticNet\"]\n",
        "for type_str in types:\n",
        "    model = sm.OLS(y_train, X_train_scaled)\n",
        "    fit = get_fit(model, type_str)\n",
        "    prediction = fit.predict(X_test_scaled)\n",
        "    print_quality(y_test, prediction, type_str)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xSFFnbewYJM"
      },
      "source": [
        "#### 3. [1 point] Explore the values of the parameters of the resulting models and compare the number of zero weights in them. Comment on the significance of the coefficients, overal model significance and other related factors from the results table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KtBv3mDuwYJM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e855a703-5e73-4d6b-dcdd-beac55c8fc42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear const    3928.681289 | x1 = 4970.821393 | x2 = -100.623988 | x3 =  181.037242 | x4 =  452.812052 | x5 = -227.657744 | x6 = -133.591532 | x7      -1273.901094 | x8 =   78.114669 | x9 =   50.601895 | dtype: float64\n",
            "Ridge const    3889.783455 | x1 = 3978.162860 | x2 = -101.146981 | x3 =  167.583345 | x4 =  470.756273 | x5 = -166.061617 | x6 = -124.026129 | x7 = -252.063228 | x8 =   66.910711 | x9 =   14.034773 | dtype: float64\n",
            "Lasso const    3928.671289 | x1 = 4787.133672 | x2 = -100.822264 | x3 =  179.157127 | x4 =  459.470274 | x5 = -215.429269 | x6 = -133.337373 | x7      -1086.947195 | x8 =   88.844953 | x9 =   34.960575 | dtype: float64\n",
            "ElasticNet const    3913.023197 | x1 = 4423.366780 | x2 = -100.676516 | x3 =  173.980640 | x4 =  464.928746 | x5 = -191.804274 | x6 = -129.593753 | x7 = -674.929709 | x8 =   56.924328 | x9 =   11.092608 | dtype: float64\n",
            "\n",
            "Linear                             OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:                  price   R-squared:                       0.874\n",
            "Model:                            OLS   Adj. R-squared:                  0.874\n",
            "Method:                 Least Squares   F-statistic:                 3.326e+04\n",
            "Date:                Sat, 08 Oct 2022   Prob (F-statistic):               0.00\n",
            "Time:                        16:05:05   Log-Likelihood:            -3.7423e+05\n",
            "No. Observations:               43152   AIC:                         7.485e+05\n",
            "Df Residuals:                   43142   BIC:                         7.486e+05\n",
            "Df Model:                           9                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "==============================================================================\n",
            "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
            "------------------------------------------------------------------------------\n",
            "const       3928.6813      6.802    577.592      0.000    3915.350    3942.013\n",
            "x1          4970.8214     32.497    152.965      0.000    4907.128    5034.515\n",
            "x2          -100.6240      7.787    -12.921      0.000    -115.887     -85.361\n",
            "x3           181.0372      6.829     26.510      0.000     167.652     194.422\n",
            "x4           452.8121      7.226     62.661      0.000     438.648     466.976\n",
            "x5          -227.6577      8.549    -26.629      0.000    -244.414    -210.901\n",
            "x6          -133.5915      8.165    -16.361      0.000    -149.596    -117.587\n",
            "x7         -1273.9011     50.821    -25.067      0.000   -1373.510   -1174.292\n",
            "x8            78.1147     30.943      2.525      0.012      17.467     138.763\n",
            "x9            50.6019     31.078      1.628      0.103     -10.311     111.515\n",
            "==============================================================================\n",
            "Omnibus:                     9391.792   Durbin-Watson:                   2.008\n",
            "Prob(Omnibus):                  0.000   Jarque-Bera (JB):           434099.928\n",
            "Skew:                          -0.007   Prob(JB):                         0.00\n",
            "Kurtosis:                      18.538   Cond. No.                         17.5\n",
            "==============================================================================\n",
            "\n",
            "Notes:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified. \n",
            "\n",
            " ============================================================ \n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# your code here\n",
        "for type_str in types:\n",
        "    model = sm.OLS(y_train, X_train_scaled)\n",
        "    fit = get_fit(model, type_str)\n",
        "    print(type_str, str(fit.params).replace(\"\\n\", \" | \").replace(\"       \", \" = \"))\n",
        "\n",
        "model = sm.OLS(y_train, X_train_scaled)\n",
        "fit = get_fit(model, \"Linear\")\n",
        "print(\"\\nLinear\", fit.summary(), \"\\n\" * 2, \"=\" * 60, \"\\n\" * 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) У всех четырех моделей кол-во нулевых (близких к нулю) коэффициентов равно нулю :) Нет нулевых коэффициентов.\n",
        "\n",
        "2) Рассмотри таблицу результатов базовой линейной модели. Общее количество наблюдений равно 43’152 c 9 степенями свободы в модели. В целом модель получилось значимой, так как Prob (F-statistic) = 0.00. Значение R-squared, которое равно 0.87, показывает какую часть изменчивости наблюдаемой переменной можно объяснить с помощью построенной модели. Значение с поправкой на степени свободы Adj R-squared также равно 0.87, что является очень хорошим результатом. \n",
        "\n",
        "Теперь выявим значимые переменные. Определенно к значимым ожно отнести переенные с x1 по x7, так как в столбце P > |t| для них указано нулевое значение. Переенная x8 значиам на уровне 1.2% и переменная x9 значима на уровне 10.3%.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dOGaRh8MC_Ca"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0A5oxV38wYJM"
      },
      "source": [
        "#### 4. [1 point] Implement one of the elimination algorithms that were described in the Seminar_4 (Elimination by P-value, Forward elimination, Backward elimination), make conclusions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wV6o20VHwYJN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb5a0cd0-a527-4c38-ea39-fc2dd8df6cfa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:                  price   R-squared:                       0.874\n",
            "Model:                            OLS   Adj. R-squared:                  0.874\n",
            "Method:                 Least Squares   F-statistic:                 3.742e+04\n",
            "Date:                Sat, 08 Oct 2022   Prob (F-statistic):               0.00\n",
            "Time:                        16:05:06   Log-Likelihood:            -3.7423e+05\n",
            "No. Observations:               43152   AIC:                         7.485e+05\n",
            "Df Residuals:                   43143   BIC:                         7.485e+05\n",
            "Df Model:                           8                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "==============================================================================\n",
            "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
            "------------------------------------------------------------------------------\n",
            "const       3928.6813      6.802    577.581      0.000    3915.349    3942.013\n",
            "x1          4971.1855     32.496    152.977      0.000    4907.492    5034.879\n",
            "x2          -100.2896      7.785    -12.883      0.000    -115.548     -85.031\n",
            "x3           180.9339      6.829     26.495      0.000     167.549     194.319\n",
            "x4           452.8370      7.226     62.664      0.000     438.673     467.001\n",
            "x5          -221.7806      7.750    -28.617      0.000    -236.971    -206.591\n",
            "x6          -133.9415      8.163    -16.409      0.000    -149.940    -117.943\n",
            "x7         -1231.0495     43.476    -28.315      0.000   -1316.264   -1145.835\n",
            "x8            84.2329     30.714      2.742      0.006      24.033     144.433\n",
            "==============================================================================\n",
            "Omnibus:                     9390.335   Durbin-Watson:                   2.009\n",
            "Prob(Omnibus):                  0.000   Jarque-Bera (JB):           433809.887\n",
            "Skew:                          -0.007   Prob(JB):                         0.00\n",
            "Kurtosis:                      18.533   Cond. No.                         13.9\n",
            "==============================================================================\n",
            "\n",
            "Notes:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
            "RMSE = 1448.2229019046774\n"
          ]
        }
      ],
      "source": [
        "# your code here \n",
        "def get_fit(X, y):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 17)\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = sm.add_constant(scaler.fit_transform(X_train))\n",
        "    X_test_scaled = sm.add_constant(scaler.transform(X_test))\n",
        "\n",
        "    model = sm.OLS(y_train, X_train_scaled)\n",
        "    fit = model.fit()\n",
        "    prediction = fit.predict(X_test_scaled)\n",
        "\n",
        "    return fit, mean_squared_error(y_test, prediction, squared = False)\n",
        "\n",
        "\n",
        "def get_min(cur_drop, cur_data):\n",
        "    min_rmse, min_drop, min_data = None, cur_drop, cur_data\n",
        "\n",
        "    for i in range(cur_data.shape[1]):\n",
        "        data_local = cur_data.drop(cur_data.columns[i - 1], axis = 1)\n",
        "        drop_local, rmse_local = get_fit(data_local, y)\n",
        "        if min_rmse is None or rmse_local <= min_rmse:\n",
        "            min_rmse, min_drop, min_data = rmse_local, drop_local, data_local\n",
        "    \n",
        "    return min_rmse, min_drop, min_data\n",
        "\n",
        "\n",
        "def backward_elimination(data, y):\n",
        "    cur_data = data\n",
        "    cur_drop, cur_rmse = get_fit(cur_data, y)\n",
        "    min_rmse, min_drop, min_data = cur_rmse, cur_drop, cur_data\n",
        "\n",
        "    while min_rmse <= cur_rmse:\n",
        "        cur_rmse, cur_drop, cur_data = min_rmse, min_drop, min_data\n",
        "        min_rmse, min_drop, min_data = get_min(cur_drop, cur_data)\n",
        "  \n",
        "    return cur_drop, cur_rmse\n",
        "\n",
        "result, rmse = backward_elimination(X, y)\n",
        "\n",
        "print(result.summary())\n",
        "print(\"RMSE =\", rmse)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Мною был реализовн алгоритм backward elimination.\n",
        "Ожидаемо, из списка переменных была \"дропнута\" x9, имевшая наибольшой значение P>|t|. После этого уровень значимости x8 улучшился.\n",
        "\n",
        "В целом результаты модели не сильно изменились, так как она и до этого показывала хороший результат. Степеней свободы теперь 8."
      ],
      "metadata": {
        "id": "6UoJLJorFjgg"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ak9mIBtlwYJN"
      },
      "source": [
        "#### 5. [1 point] Find the best (in terms of RMSE) $\\alpha$ for Lasso regression using cross-validation with 4 folds. You must select values from range $[10^{-4}, 10^{3}]$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oTvz-2zRwYJN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91ae1dba-2972-4f45-a9ed-5bd164bbec17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Лучший результат с альфа = 3.727593720314938\n"
          ]
        }
      ],
      "source": [
        "# your code here\n",
        "model = GridSearchCV(Lasso(), [{\"alpha\": np.logspace(-4, 3)}], cv = 4, scoring = \"neg_root_mean_squared_error\")\n",
        "model.fit(X_train_scaled, y_train)\n",
        "print(\"Лучший результат с альфа =\", model.best_params_[\"alpha\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWd2CzcrwYJN"
      },
      "source": [
        "## Gradient descent\n",
        "\n",
        "#### 6. [3.5 points] Implement a Ridge regression model for the MSE loss function, trained by gradient descent.\n",
        "\n",
        "All calculations must be vectorized, and python loops can only be used for gradient descent iterations. As a stop criterion, you must use (simultaneously):\n",
        "\n",
        "* checking for the Absolute-value norm of the weight difference on two adjacent iterations (for example, less than some small number of the order of $10^{-6}$, set by the `tolerance` parameter);\n",
        "* reaching the maximum number of iterations (for example, 10000, set by the `max_iter` parameter).\n",
        "\n",
        "You need to implement:\n",
        "\n",
        "* Full gradient descent:\n",
        "\n",
        "$$\n",
        "w_{k + 1} = w_{k} - \\eta_{k} \\nabla_{w} Q(w_{k}).\n",
        "$$\n",
        "\n",
        "* Stochastic Gradient Descent:\n",
        "\n",
        "$$\n",
        "w_{k + 1} = w_{k} - \\eta_{k} \\nabla_{w} q_{i_{k}}(w_{k}).\n",
        "$$\n",
        "\n",
        "$\\nabla_{w} q_{i_{k}}(w_{k}) \\, $ is the estimate of the gradient over the batch of objects selected randomly.\n",
        "\n",
        "* Momentum method:\n",
        "\n",
        "$$\n",
        "h_0 = 0, \\\\\n",
        "h_{k + 1} = \\alpha h_{k} + \\eta_k \\nabla_{w} Q(w_{k}), \\\\\n",
        "w_{k + 1} = w_{k} - h_{k + 1}.\n",
        "$$\n",
        "\n",
        "* Adagrad method:\n",
        "\n",
        "$$\n",
        "G_0 = 0, \\\\\n",
        "G_{k + 1} = G_{k} + (\\nabla_{w} Q(w_{k+1}))^2, \\\\\n",
        "w_{k + 1} = w_{k} - \\eta * \\frac{\\nabla_{w} Q(w_{k+1})}{\\sqrt{G_{k+1} + \\epsilon}}.\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "To make sure that the optimization process really converges, we will use the `loss_history` class attribute. After calling the `fit` method, it should contain the values of the loss function for all iterations, starting from the first one (before the first step on the anti-gradient).\n",
        "\n",
        "You need to initialize the weights with a random vector from normal distribution. The following is a template class that needs to contain the code implementing all variations of the models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lzSxJjDSwYJN"
      },
      "outputs": [],
      "source": [
        "from sklearn.base import BaseEstimator\n",
        "import math\n",
        "\n",
        "'''\n",
        "Пользовался этой статьей\n",
        "https://machinelearningcompass.com/machine_learning_models/ridge_regression/\n",
        "\n",
        "И вот этим видео\n",
        "https://www.youtube.com/watch?v=C98SRCZfgkk\n",
        "\n",
        "И вот этой статьей\n",
        "https://d2l.ai/chapter_optimization/adagrad.html\n",
        "'''\n",
        "\n",
        "class MyRidge(BaseEstimator):\n",
        "    def __init__(self, delta=1.0, gd_type='Momentum', tolerance=1e-4,\n",
        "                 max_iter=1000, w0=None, eta=1e-2, alpha=1e-3, epsilon = 1):\n",
        "        \"\"\"\n",
        "        gd_type: str\n",
        "            'GradientDescent', 'StochasticDescent', 'Momentum', 'Adagrad'\n",
        "        delta: float\n",
        "            proportion of object in a batch (for stochastic GD)\n",
        "        tolerance: float\n",
        "            for stopping gradient descent\n",
        "        max_iter: int\n",
        "            maximum number of steps in gradient descent\n",
        "        w0: np.array of shape (d)\n",
        "            init weights\n",
        "        eta: float\n",
        "            learning rate\n",
        "        alpha: float\n",
        "            momentum coefficient\n",
        "        reg_cf: float\n",
        "            regularization coefficient\n",
        "        epsilon: float\n",
        "            numerical stability\n",
        "        \"\"\"\n",
        "        \n",
        "        self.delta = delta\n",
        "        self.gd_type = gd_type\n",
        "        self.tolerance = tolerance\n",
        "        self.max_iter = max_iter\n",
        "        self.w0 = w0\n",
        "        self.alpha = alpha\n",
        "        self.w = None\n",
        "        self.eta = eta\n",
        "        self.loss_history = None\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "        self.h = 0\n",
        "        self.g = 0\n",
        "        self.gd_functions = {'GradientDescent': self.gradient_descent,\n",
        "                             'StochasticDescent': self.stochastic_descent,\n",
        "                             'Momentum': self.momentum,\n",
        "                             'Adagrad': self.adagrad}\n",
        "    \n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        X: np.array of shape (l, d)\n",
        "        y: np.array of shape (l)\n",
        "        ---\n",
        "        output: self\n",
        "        \"\"\"\n",
        "        self.loss_history = []\n",
        "        self.w = self.w0\n",
        "\n",
        "        cur_iteration = 0\n",
        "        while cur_iteration < self.max_iter and (cur_iteration == 0 or self.tolerance < self.loss_history[-1]):\n",
        "            weight = self.gd_functions[self.gd_type](X, y)\n",
        "            self.w, weight = weight, self.w\n",
        "            self.loss_history.append(self.calc_loss(X, y))\n",
        "            cur_iteration += 1\n",
        "\n",
        "        return self\n",
        "    \n",
        "    def predict(self, X):\n",
        "        return X.dot(np.transpose(self.w))\n",
        "    \n",
        "    def calc_gradient(self, X, y):\n",
        "        return 2 * np.dot(X.T, self.predict(X) - y) / y.shape[0]\n",
        "\n",
        "    def calc_loss(self, X, y):\n",
        "        rss = np.sum(np.power(self.predict(X) - y, 2))\n",
        "        penalty = np.sum(np.power(self.w, 2))\n",
        "        return np.mean(rss + penalty)\n",
        "    \n",
        "    # -----------------------------------------------------------------------\n",
        "    \n",
        "    def gradient_descent(self, X, y):\n",
        "        return self.w - self.eta * self.calc_gradient(X, y)\n",
        "        \n",
        "    def stochastic_descent(self, X, y):\n",
        "        index = np.random.randint(0, len(X), math.floor(self.delta * len(X)))\n",
        "        return self.w - self.eta * self.calc_gradient(X[index, :], y.to_numpy()[index])\n",
        "        \n",
        "    def momentum(self, X, y):\n",
        "        self.h = self.alpha * self.h + self.eta * self.calc_gradient(X, y)\n",
        "        return self.w - self.h\n",
        "\n",
        "    def adagrad(self, X, y):\n",
        "        self.g = self.g + np.power(self.calc_gradient(X, y), 2)\n",
        "        return self.w - self.eta * self.calc_gradient(X, y) / np.sqrt(self.g + self.epsilon)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qa4y11OxwYJO"
      },
      "source": [
        "#### 7. [1 points] Train and validate \"hand-written\" models on the same data, and compare the quality with the Sklearn or StatsModels methods. Investigate the effect of the `max_iter` and `alpha` parameters on the optimization process. Is it consistent with your expectations?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MWzW9-pswYJO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04b3ea05-35bd-460c-e7f9-1c6669dd1de6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SK] RMSE = 1448.2313052658506, r2 = 0.8704814590613568\n",
            "[MY] RMSE = 1499.6082828068595, r2 = 0.8611289426336216\n",
            "\n",
            "[ITER = 100] RMSE = 1753.0857600372515, r2 = 0.8102147722448776\n",
            "[ITER = 500] RMSE = 1563.178759960914, r2 = 0.84910551334503\n",
            "[ITER = 1000] RMSE = 1499.6082828068595, r2 = 0.8611289426336216\n",
            "[ITER = 2000] RMSE = 1459.7729380638327, r2 = 0.8684088453348653\n",
            "[ITER = 5000] RMSE = 1448.1444649338687, r2 = 0.870496991243042\n",
            "\n",
            "[ALPHA = 0] RMSE = 1499.6082828068595, r2 = 0.8611289426336216\n",
            "[ALPHA = 0.01] RMSE = 1498.7927116436017, r2 = 0.8612799533111883\n",
            "[ALPHA = 0.05] RMSE = 1495.505670176956, r2 = 0.8618877472148415\n",
            "[ALPHA = 0.1] RMSE = 1491.350448928271, r2 = 0.8626541631694599\n",
            "[ALPHA = 0.5] RMSE = 1459.7510047372784, r2 = 0.8684127996622933\n",
            "[ALPHA = 0.9] RMSE = 1447.81555362485, r2 = 0.8705558115720164\n",
            "[ALPHA = 0.99] RMSE = 1448.5553492163947, r2 = 0.8704234926371899\n",
            "[ALPHA = 0.995] RMSE = 1482.938907533331, r2 = 0.8641991147917698\n",
            "[ALPHA = 0.999] RMSE = 2967.143594727026, r2 = 0.4563327025604915\n",
            "[ALPHA = 1] RMSE = 4615.331265224075, r2 = -0.3154121141743429\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# your code here\n",
        "\n",
        "def model_results(model, name):\n",
        "    trained = model.fit(X_train_scaled, y_train)\n",
        "    prediction = trained.predict(X_test_scaled)\n",
        "    rmse = mean_squared_error(y_test, prediction, squared = False)\n",
        "    r2 = r2_score(y_test, prediction)\n",
        "    print(f\"[{name}] RMSE = {rmse}, r2 = {r2}\")\n",
        "\n",
        "random_w0 = np.random.rand(X_train_scaled[0].size)\n",
        "sk_model = sklearn.linear_model.Ridge()\n",
        "my_model = MyRidge(gd_type = 'GradientDescent', w0 = random_w0)\n",
        "model_results(sk_model, \"SK\")\n",
        "model_results(my_model, \"MY\")\n",
        "print()\n",
        "\n",
        "max_iters = [100, 500, 1000, 2000, 5000]\n",
        "for cur_iter in max_iters:\n",
        "    iter_model = MyRidge(gd_type = 'GradientDescent', w0 = random_w0, max_iter = cur_iter)\n",
        "    model_results(iter_model, f\"ITER = {cur_iter}\")\n",
        "print()\n",
        "\n",
        "\n",
        "alphas = [0, 0.01, 0.05, 0.1, 0.5, 0.9, 0.99, 0.995, 0.999, 1]\n",
        "for cur_alpha in alphas:\n",
        "    alpha_model = MyRidge(gd_type = 'Momentum', w0 = random_w0, alpha = cur_alpha)\n",
        "    model_results(alpha_model, f\"ALPHA = {cur_alpha}\")\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) Результаты мною реализовнной Ridge модели слегка хуже аналогичной в Sklearn\n",
        "Мой RMSE = 1499, sklearn rmse = 1448\n",
        "Мой r2 = 0.86, sklearn r2 = 0.87\n",
        "\n",
        "2) Кол-во итераций предсказуеом влияет на результат. Чем больше итераций, тем он лучше (rmse уменьшается, r2 растет). Кстиати, при 5000 итераций, мой класс показывает лучше результаты, чем Sklearn.\n",
        "\n",
        "3) При росте параматера альфа (тип = momentum) от 0 до 0.9 результаты улучшаются. Далее при альфа от 0.9 до 0.99 медленно начинают ухудашаться и после 0.99 до 1 происходит резкое падение r2 и огромный рост RMSE. Это объясняется тем, что чем сильнее альфа приблежается к единице, тем больше начинают влиять на конечный результат предудыщие значения self.h."
      ],
      "metadata": {
        "id": "VvhBughw9LQ4"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-uFOvq2wYJO"
      },
      "source": [
        "#### 8. [1 points] Plot graphs (on the same picture) of the dependence of the loss function value on the iteration number for Full GD, SGD, Momentum and Adagrad. Draw conclusions about the rate of convergence of various modifications of gradient descent.\n",
        "\n",
        "Don't forget about what *beautiful* graphics should look like!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KWPs8OXJwYJP"
      },
      "outputs": [],
      "source": [
        "# your code here\n",
        "\n",
        "types = {'GradientDescent': \"blue\",\n",
        "        'StochasticDescent': \"green\",\n",
        "        'Momentum': \"red\",\n",
        "        'Adagrad': \"yellow\"}\n",
        "types.pop(\"Adagrad\")\n",
        "\n",
        "plt.figure(figsize=(30, 10))\n",
        "for cur_type, color in types.items():\n",
        "    model = MyRidge(gd_type = cur_type, w0 = random_w0, delta = 0.2, alpha = 0.3, epsilon = 0.000001)\n",
        "    trained = model.fit(X_train_scaled, y_train)\n",
        "    plt.plot(trained.loss_history[:150], label = cur_type, color = color)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Общий характер графиков похож. Все методы примерно за 40-50 итерация снижают потери до 0.2 * 10 ^ 12 единиц. При этом gradient descent и stochastic descent очень похожи. Momentum показывает слегка большую эффективность, чем два предыдущих метода, при этом плавность данной кривой напрямую зависит от параметра alpha. Если начать его изменять, то график начнет прыгать и вырисовывать \"горы\"."
      ],
      "metadata": {
        "id": "mydgsDXmmqD5"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}