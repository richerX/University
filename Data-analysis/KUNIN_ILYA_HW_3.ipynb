{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3v5HIUdDvY5"
      },
      "source": [
        "# HSE 2021: Mathematical Methods for Data Analysis\n",
        "\n",
        "## Homework 3\n",
        "\n",
        "**Warning 1**: some problems require (especially the lemmatization part) significant amount of time, so **it is better to start early (!)**\n",
        "\n",
        "**Warning 2**: it is critical to describe and explain what you are doing and why, use markdown cells"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F7t9dYtdDvZC"
      },
      "outputs": [],
      "source": [
        "from typing import Tuple, List\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "sns.set(style=\"darkgrid\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIHtwV6vDvZD"
      },
      "source": [
        "## PART 1: Logit model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7XKEWcVDvZD"
      },
      "source": [
        "We consider a binary classification problem. For prediction, we would like to use a logistic regression model. For regularization we add a combination of the $l_2$ and $l_1$ penalties (Elastic Net). \n",
        "\n",
        "Each object in the training dataset is indexed with $i$ and described by pair: features $x_i\\in\\mathbb{R}^{K}$ and binary labels $y_i$. The model parametrized with bias $w_0\\in\\mathbb{R}$ and weights $w\\in\\mathbb{R}^K$.\n",
        "\n",
        "The optimization problem with respect to the $w_0, w$ is the following (Elastic Net Loss):\n",
        "\n",
        "$$L(w, w_0) = \\frac{1}{N} \\sum_{i=1}^N \\ln(1+\\exp(-y_i(w^\\top x_i+w_0))) + \\gamma \\|w\\|_1 + \\beta \\|w\\|_2^2$$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1eSuDKXFVZu"
      },
      "source": [
        "#### 1. [0.5 points]  Find the gradient of the Elastic Net loss and write its formulas (better in latex format) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zjH-YnPDvZD"
      },
      "source": [
        "$$A = {\\exp(-y_i(w^\\top x_i+w_0)}$$\n",
        "\n",
        "$$\\nabla_wL(w, w_0) = -\\frac{1}{N} \\sum_{i=1}^N y_ix_i\\frac{A}{A+1} + \\gamma sign(w) + 2\\beta w$$\n",
        "\n",
        "$$\\nabla_{w_0}L(w, w_0) = -\\frac{1}{N} \\sum_{i=1}^N y_i\\frac{A}{A+1}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_lIccN_DvZE"
      },
      "source": [
        "#### 2. [0.25 points] Implement the Elastic Net loss (as a function)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9QNfCtV5DvZE"
      },
      "outputs": [],
      "source": [
        "def loss(X, y, w: List[float], w0: float, gamma=1., beta=1.) -> float:\n",
        "    A = np.exp(-y * (np.dot(X, w.T) + w0))\n",
        "    result = np.sum(np.log(A + 1)) / len(X) + gamma * np.sum(np.abs(w)) + beta * np.linalg.norm(w)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIVoC6UmDvZE"
      },
      "source": [
        "#### 3. [0.25 points] Implement the gradient (as a function)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HWqBLGRADvZE"
      },
      "outputs": [],
      "source": [
        "def get_grad(X, y, w: List[float], w0: float, gamma=1., beta=1.) -> Tuple[List[float], float]:\n",
        "    A = np.exp(-y * (np.dot(X, w.T) + w0))\n",
        "    grad_w = - np.dot(A / (A + 1) * y, X) / len(X) + gamma * np.sign(w) + 2 * beta * w\n",
        "    grad_w0 = - np.sum(A / (A + 1) * y) / len(X)\n",
        "    return grad_w, grad_w0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhOb8HrtDvZF"
      },
      "source": [
        "#### Check yourself"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3FxXTocHDvZF"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "X = np.random.multivariate_normal(np.arange(5), np.eye(5), size=10)\n",
        "y = np.random.binomial(1, 0.42, size=10)\n",
        "w, w0 = np.random.normal(size=5), np.random.normal()\n",
        "\n",
        "grad_w, grad_w0 = get_grad(X, y, w, w0)\n",
        "assert(np.allclose(grad_w,\n",
        "                   [-2.73262076, -1.87176281, 1.30051144, 2.53598941, -2.71198109],\n",
        "                   rtol=1e-2) & \\\n",
        "       np.allclose(grad_w0,\n",
        "                   -0.2078231418067844, \n",
        "                   rtol=1e-2)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbqLfcrRDvZF"
      },
      "source": [
        "####  4. [1 point]  Implement gradient descent which works for both tol level and max_iter stop criteria and plot the decision boundary of the result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIgiwQkjDvZF"
      },
      "source": [
        "The template provides basic sklearn API class. You are free to modify it in any convenient way."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Thyeux0KDvZG"
      },
      "outputs": [],
      "source": [
        "from sklearn.base import BaseEstimator, ClassifierMixin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "erVmNR0PDvZG"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_curve\n",
        "\n",
        "\n",
        "class Logit(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self, beta=1.0, gamma=1.0, learning_rate=1e-3, tolerance=0.01, max_iter=1000, random_state=42, threshold=0.5):  \n",
        "        self.beta = beta        \n",
        "        self.gamma = gamma\n",
        "        self.tolerance = tolerance\n",
        "        self.max_iter = max_iter\n",
        "        self.learning_rate = learning_rate\n",
        "        self.random_state = random_state\n",
        "        # you may additional properties if you wish\n",
        "\n",
        "        # Флаг на наличие нуля и история лоссов\n",
        "        self.flag = False\n",
        "        self.loss_history = []\n",
        "\n",
        "        # Названия взяты от roc_curve из sklearn.metrics\n",
        "        # https://habr.com/ru/company/netologyru/blog/582756/\n",
        "        self.fpr, self.tpr = 0, 0\n",
        "        self.threshold = threshold\n",
        "        \n",
        "    def fit(self, X, y):\n",
        "        # add weights and bias and optimize Elastic Net loss over (X,y) dataset\n",
        "        # save history of optimization steps\n",
        "\n",
        "        self.w0 = 1\n",
        "        self.w = np.ones(len(X[0]))\n",
        "\n",
        "        if np.count_nonzero(y) < len(y):\n",
        "            self.flag = True\n",
        "            y[y == 0] = -1\n",
        "        \n",
        "        for i in range(self.max_iter):\n",
        "            grad_w, grad_w0 = get_grad(X, y, self.w, self.w0, self.gamma, self.beta)\n",
        "            self.w = self.w - grad_w * self.learning_rate\n",
        "            self.w0 = self.w0 - grad_w0 * self.learning_rate\n",
        "            self.loss_history.append(loss(X, y, self.w, self.w0, self.gamma, self.beta))\n",
        "            self.fpr, self.tpr, array = roc_curve(y, self.predict_proba(X)[1,:])\n",
        "            self.threshold = array[np.argmax(self.tpr - self.fpr)]\n",
        "\n",
        "        return self\n",
        "    \n",
        "    def predict(self, X):\n",
        "        # return vector of predicted labels for each object from X\n",
        "\n",
        "        predicted_proba = self.predict_proba(X)\n",
        "        predict = np.ones(predicted_proba.shape[1])\n",
        "\n",
        "        predict[self.threshold > predicted_proba[1, :]] = 0\n",
        "        if self.flag == False:\n",
        "            predict[self.threshold > predicted_proba[1, :]] = -1\n",
        "        \n",
        "        return predict\n",
        "        \n",
        "    def predict_proba(self, X):\n",
        "        return np.array([1 / (1 + np.exp(np.dot(X, self.w) + self.w0)),\\\n",
        "                         1 / (1 + np.exp(-np.dot(X, self.w) - self.w0))])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7SJX8Y6EDvZG"
      },
      "outputs": [],
      "source": [
        "# sample data to test your model\n",
        "from sklearn.datasets import make_classification\n",
        "X, y = make_classification(n_samples=180, n_features=2, n_redundant=0, n_informative=2,\n",
        "                               random_state=42, n_clusters_per_class=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u41kzwGTDvZH"
      },
      "outputs": [],
      "source": [
        "# a function to plot the decision boundary\n",
        "def plot_decision_boundary(model, X, y):\n",
        "    fig = plt.figure()\n",
        "    X1min, X2min = X.min(axis=0)\n",
        "    X1max, X2max = X.max(axis=0)\n",
        "    x1, x2 = np.meshgrid(np.linspace(X1min, X1max, 200),\n",
        "                         np.linspace(X2min, X2max, 200))\n",
        "    ypred = model.predict(np.c_[x1.ravel(), x2.ravel()])\n",
        "    ypred = ypred.reshape(x1.shape)\n",
        "    \n",
        "    plt.contourf(x1, x2, ypred, alpha=.4)\n",
        "    plt.scatter(X[:,0], X[:,1], c=y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mNuYbsAoDvZI"
      },
      "outputs": [],
      "source": [
        "model = Logit(0,0)\n",
        "y[y == 0] = -1\n",
        "model.fit(X, y)\n",
        "plot_decision_boundary(model, X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qi4WRhcADvZI"
      },
      "source": [
        "#### 5. [0.25 points] Plot loss diagram for the model, i.e. show the dependence of the loss function from the gradient descent steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VyjMDAKuDvZI"
      },
      "outputs": [],
      "source": [
        "plt.plot(model.loss_history)\n",
        "plt.xlabel(\"Итерации\")\n",
        "plt.ylabel(\"Потери\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Поднял learning rate, потери сильно снизились\n",
        "my_model = Logit(0, 0, learning_rate = 0.1)\n",
        "y[y == 0] = -1\n",
        "my_model.fit(X, y)\n",
        "\n",
        "plt.plot(my_model.loss_history)\n",
        "plt.xlabel(\"Итерации\")\n",
        "plt.ylabel(\"Потери\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZYnmOMlVh23p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FhSCAv_DvZJ"
      },
      "source": [
        "## PART 2: Support Vector Machines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYyGsSxEDvZJ"
      },
      "source": [
        "#### 6. [2 point] Using the same dataset, train SVM Classifier from Sklearn.\n",
        "Investigate how different parameters influence the quality of the solution:\n",
        "+ Try several kernels: Linear, Polynomial, RBF (and others if you wish). Some Kernels have hypermeters: don't forget to try different.\n",
        "+ Regularization coefficient \n",
        "\n",
        "Show how these parameters affect accuracy, roc_auc and f1 score. \n",
        "Make plots for the dependencies between metrics and parameters. \n",
        "Try to formulate conclusions from the observations. How sensitive are kernels to hyperparameters? How sensitive is a solution to the regularization? Which kernel is prone to overfitting?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nicu_O3IDvZK"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score\n",
        "\n",
        "\n",
        "def get_score(kernel, regularization, score, degree = None, gamma = None, use_x_train = False):\n",
        "    if degree is not None:\n",
        "        model = SVC(kernel = kernel, C = regularization, degree = degree)\n",
        "    elif gamma is not None:\n",
        "        model = SVC(kernel = kernel, C = regularization, gamma = gamma)\n",
        "    else:\n",
        "        model = SVC(kernel = kernel, C = regularization)\n",
        "    \n",
        "    model.fit(X_train, y_train)\n",
        "    if use_x_train:\n",
        "        current_y = y_train\n",
        "        prediciton = model.predict(X_train)\n",
        "    else:\n",
        "        current_y = y_test\n",
        "        prediciton = model.predict(X_test)\n",
        "\n",
        "    if score == \"accuracy\":\n",
        "        return accuracy_score(current_y, prediciton)\n",
        "    elif score == \"roc\":\n",
        "        return roc_auc_score(current_y, prediciton)\n",
        "    elif score == \"f1\":\n",
        "        return f1_score(current_y, prediciton)\n",
        "\n",
        "\n",
        "def axis_settings(axis, title, xlabel, ylabel, legend, xscale):\n",
        "    if title:\n",
        "        axis.set_title(title)\n",
        "    if xlabel:\n",
        "        axis.set_xlabel(xlabel)\n",
        "    if ylabel:\n",
        "        axis.set_ylabel(score)\n",
        "    if legend:\n",
        "        axis.legend()\n",
        "    if xscale:\n",
        "        axis.set_xscale(xscale)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
        "figsize = (24, 6)\n",
        "\n",
        "# Общие графики без использования гиперпараметров\n",
        "scores = [\"accuracy\", \"roc\", \"f1\"]\n",
        "kernels = [\"linear\", \"poly\", \"rbf\"]\n",
        "regularizations = [0.01, 0.1, 1, 10, 100]\n",
        "figures, axis = plt.subplots(1, 3, figsize = figsize)\n",
        "for index, score in enumerate(scores):\n",
        "    for kernel in kernels:\n",
        "        current = []\n",
        "        for regularization in regularizations:\n",
        "            current.append(get_score(kernel, regularization, score))\n",
        "        axis[index].plot(regularizations, current, label = kernel)\n",
        "    axis_settings(axis[index], f\"Kernel = {kernel}\", \"regularization\", score, True, \"log\")\n",
        "plt.show()\n",
        "\n",
        "# График тестирования различных значений степени для POLY ядра\n",
        "# и график тестирования различных значений гаммы для RBF ядра\n",
        "figures, axis = plt.subplots(1, 2, figsize = figsize)\n",
        "degrees = [0, 1, 2, 3, 4, 5]\n",
        "gammas = [0.0001, 0.001, 0.01, 0.1, 1]\n",
        "colors = [\"red\", \"yellow\", \"purple\"]\n",
        "\n",
        "for index, score in enumerate(scores):\n",
        "    current = []\n",
        "    for degree in degrees:\n",
        "        current.append(get_score(\"poly\", 1, score, degree = degree))\n",
        "    axis[0].plot(degrees, current, label = score, color = colors[index])\n",
        "\n",
        "for index, score in enumerate(scores):\n",
        "    current = []\n",
        "    for gamma in gammas:\n",
        "        current.append(get_score(\"rbf\", 1, score, gamma = gamma))\n",
        "    axis[1].plot(gammas, current, label = score, color = colors[index])\n",
        "\n",
        "axis_settings(axis[0], \"Kernel = poly\", \"degree\", \"scores\", True, None)\n",
        "axis_settings(axis[1], \"Kernel = rbf\", \"gamma\", \"scores\", True, \"log\")\n",
        "plt.show()\n",
        "\n",
        "# Графики для проверки prone overfitting\n",
        "score = \"accuracy\"\n",
        "kernels = [\"linear\", \"poly\", \"rbf\"]\n",
        "regularizations = [0.01, 0.1, 1, 10, 100]\n",
        "figures, axis = plt.subplots(1, 3, figsize = figsize)\n",
        "for index, kernel in enumerate(kernels):\n",
        "    current1, current2 = [], []\n",
        "    for regularization in regularizations:\n",
        "        current1.append(get_score(kernel, regularization, score))\n",
        "        current2.append(get_score(kernel, regularization, score, use_x_train = True))\n",
        "    axis[index].plot(regularizations, current1, label = kernel + \" test\")\n",
        "    axis[index].plot(regularizations, current2, label = kernel + \" train\", linestyle = 'dashed')\n",
        "    axis_settings(axis[index], f\"Kernel = {kernel}\", \"regularization\", score, True, \"log\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) Рассмотрим графики различных score для разных ядер в зависимсоти от \n",
        "regularization параметра.\n",
        "\n",
        "Scores: accuracy, roc и f1. <br>\n",
        "Ядра: linear, poly, rbf. <br>\n",
        "Regularization: от 10^(-2) до 10^(2).\n",
        "\n",
        "По оси x: регуляризация. <br>\n",
        "По оси y: один из scores.\n",
        "\n",
        "В целом, все 3 графика похожи. При 10^(-2) rbf показывает сильно хуже результаты, чем два остальных ядра. При 10^(-1) все три ядра начинают показывать схожие результаты. После 10^(0) все ядра достигают максимума (или практически достигают) и в дальнейшем резких изменений нет. Максимум по всем параметрам от 0,97 до 1.\n",
        "\n",
        "---\n",
        "\n",
        "2) Рассмотрим графики accuracy в зависимости от различных гиперпараметров для разных ядер.\n",
        "\n",
        "Regularization: 1. <br>\n",
        "Degree (poly): от 0 до 5. <br>\n",
        "Gamma (rbf): от 10^(-4) до 10^(0).\n",
        "\n",
        "По оси x: гиперпараметры. <br>\n",
        "По оси y: scores.\n",
        "\n",
        "Сначала рассмотрим график poly ядра. При degree = 0 результаты низкие. При degree = 1 метрики резко возрастают и практически достигают максимума. При degree от 1 до 4 резльутаты практически не меняются. При degree = 5 и далее метрики снижаются.\n",
        "\n",
        "Сначала рассмотрим график rbf ядра. При gamma = 10^(-4) результаты низкие. Далее до gamma = 10^(-1) метрики постепенно возрастают и достигают максимума. Далее график движется практически горизонтально.\n",
        "\n",
        "---\n",
        "\n",
        "3) Рассмотрим графики показывающие точность предсказаний ядер на test данных на train данных.\n",
        "\n",
        "Ядра: linear, poly, rbf. <br>\n",
        "Data: train, test.\n",
        "\n",
        "По оси x: regularization. <br>\n",
        "По оси y: accuracy.\n",
        "\n",
        "Нас интересуют ситуации, когда модель на test данных показывает результаты хуже, чем на train данных. В данном случае явно видно, что переобучена модель c linear ядром. Модели с ядрами poly и rbf показывают примерно равный accuracy для train и test данных.\n",
        "\n",
        "Наиболее склонное к переобучению - линейное ядро."
      ],
      "metadata": {
        "id": "_2nijL44B08K"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sY8q6JdCDvZK"
      },
      "source": [
        "## PART 3: Natural Language Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eD4xKhYfDvZK"
      },
      "source": [
        "#### 7. [1.75 point] Form the dataset\n",
        "\n",
        "We are going to form a dataset that we will use in the following tasks for binary and multiclass classification\n",
        "\n",
        "0. Choose **six** authors that you like (specify who you've chosen) and download the <a href=\"https://www.kaggle.com/d0rj3228/russian-literature?select=prose\">relevant data</a> from **prose** section\n",
        "1. Build your own dataset for these authors: \n",
        "    * divide each text into sentences such that we will have two columns: *sentence* and *target author*, each row will contain one sentence and one target\n",
        "    * drop sentences where N symbols in a sentence < 15\n",
        "    * fix random state and randomly choose sentences in the folowing proportion \"5k : 15k : 8k : 11k : 20k : 3k\" for the authors respectively\n",
        "    \n",
        "    sample data may look like:\n",
        "    \n",
        "    <center> \n",
        "    <table>\n",
        "        <tr>\n",
        "            <th> sentence </th>\n",
        "            <th> author </th>\n",
        "        </tr> \n",
        "        <tr><td> Несколько лет тому назад в одном из своих поместий жил старинный русской барин, Кирила Петрович Троекуров. </td><td> Пушкин </td><td> \n",
        "        <tr><td> Уже более недели приезжий господин жил в городе, разъезжая по вечеринкам и обедам и таким образом проводя, как говорится, очень приятно время. </td><td> Гоголь </td><td> \n",
        "        <tr><td> ... </td><td> ... </td><td> \n",
        "        <tr><td> Я жил недорослем, гоняя голубей и играя в чехарду с дворовыми мальчишками. </td><td> Пушкин </td><td>         \n",
        "    </table>\n",
        "</center>\n",
        "     \n",
        "2. Preprocess (tokenize and clean) the dataset \n",
        "    * tokenize, remove all stop words (nltk.corpus.stopwords), punctuation (string.punctuation) and numbers\n",
        "    * convert to lower case and apply either stemming or lemmatization of the words (on your choice)\n",
        "    * vectorize words using both **bag of words** and **tf-idf** (use sklearn)\n",
        "    * observe and describe the difference between vectorized output (what do numbers look like after transformations and what do they represent?)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymorphy2\n",
        "\n",
        "import os\n",
        "import re\n",
        "import random\n",
        "import pymorphy2\n",
        "import nltk\n",
        "import nltk.data\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "7vYN9zJy_tsB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NVStbeQ8DvZL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "outputId": "8708fb7b-cb46-4118-b1d4-b8d13c0f8fc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Самые популярные токены CountVectorizer: ['весь', 'говорить', 'знать', 'который', 'один', 'рука', 'свой', 'сказать', 'человек', 'это']\n",
            "Самые популярные токены TfidfVectorizer: ['весь', 'говорить', 'знать', 'который', 'один', 'рука', 'свой', 'сказать', 'человек', 'это']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                            sentence      author  \\\n",
              "0   Тушар в ответ ей пожал плечами, что, конечно,...  Dostoevsky   \n",
              "1   Вот почему, и только поэтому, я обращал на не...  Dostoevsky   \n",
              "2   тогда как – все призрак, и мираж, и ложь, и с...  Dostoevsky   \n",
              "3   Наполеон вздрогнул, подумал и сказал мне: «Ты...  Dostoevsky   \n",
              "4                     Смятение души его вдруг прошло  Dostoevsky   \n",
              "5              Кто ж я на земле, как не приживальщик  Dostoevsky   \n",
              "6  ) и проявляться не может иначе как сурово, гор...  Dostoevsky   \n",
              "7   Ну что, если б каждый из них вдруг узнал весь...  Dostoevsky   \n",
              "8   В заключение скажу, что вами означенный долг ...  Dostoevsky   \n",
              "9   — с достоинством и благородством заключил наш...  Dostoevsky   \n",
              "\n",
              "                                              lemmas  \\\n",
              "0  тушар ответ пожать плечо означать недаром деск...   \n",
              "1  почему поэтому обращать внимание следить она э...   \n",
              "2  призрак мираж ложь стыд неестественность мера ...   \n",
              "3  наполеон вздрогнуть подумать сказать напомнить...   \n",
              "4                               смятение душа пройти   \n",
              "5                                 земля приживальщик   \n",
              "6  проявляться иначе сурово горячо круто часто по...   \n",
              "7                        б каждый узнать весь секрет   \n",
              "8  заключение сказать вы означенный долг рубль се...   \n",
              "9       достоинство благородство заключить наш герой   \n",
              "\n",
              "                  CountVectorizer  \\\n",
              "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
              "1  [0, 0, 2, 0, 0, 0, 0, 0, 0, 1]   \n",
              "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 2]   \n",
              "3  [0, 0, 0, 1, 0, 0, 0, 1, 0, 0]   \n",
              "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
              "5  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
              "6  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
              "7  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
              "8  [1, 0, 0, 0, 0, 0, 0, 1, 0, 0]   \n",
              "9  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
              "\n",
              "                                     TfidfVectorizer  \n",
              "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
              "1  [0.0, 0.0, 0.9243488761648889, 0.0, 0.0, 0.0, ...  \n",
              "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
              "3  [0.0, 0.0, 0.0, 0.7191985373641532, 0.0, 0.0, ...  \n",
              "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
              "5  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
              "6  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
              "7  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
              "8  [0.77854435946222, 0.0, 0.0, 0.0, 0.0, 0.0, 0....  \n",
              "9  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-830e1ee6-76c4-4e5b-a64e-9f4e13bcbf74\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>author</th>\n",
              "      <th>lemmas</th>\n",
              "      <th>CountVectorizer</th>\n",
              "      <th>TfidfVectorizer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Тушар в ответ ей пожал плечами, что, конечно,...</td>\n",
              "      <td>Dostoevsky</td>\n",
              "      <td>тушар ответ пожать плечо означать недаром деск...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Вот почему, и только поэтому, я обращал на не...</td>\n",
              "      <td>Dostoevsky</td>\n",
              "      <td>почему поэтому обращать внимание следить она э...</td>\n",
              "      <td>[0, 0, 2, 0, 0, 0, 0, 0, 0, 1]</td>\n",
              "      <td>[0.0, 0.0, 0.9243488761648889, 0.0, 0.0, 0.0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>тогда как – все призрак, и мираж, и ложь, и с...</td>\n",
              "      <td>Dostoevsky</td>\n",
              "      <td>призрак мираж ложь стыд неестественность мера ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 2]</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Наполеон вздрогнул, подумал и сказал мне: «Ты...</td>\n",
              "      <td>Dostoevsky</td>\n",
              "      <td>наполеон вздрогнуть подумать сказать напомнить...</td>\n",
              "      <td>[0, 0, 0, 1, 0, 0, 0, 1, 0, 0]</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.7191985373641532, 0.0, 0.0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Смятение души его вдруг прошло</td>\n",
              "      <td>Dostoevsky</td>\n",
              "      <td>смятение душа пройти</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Кто ж я на земле, как не приживальщик</td>\n",
              "      <td>Dostoevsky</td>\n",
              "      <td>земля приживальщик</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>) и проявляться не может иначе как сурово, гор...</td>\n",
              "      <td>Dostoevsky</td>\n",
              "      <td>проявляться иначе сурово горячо круто часто по...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Ну что, если б каждый из них вдруг узнал весь...</td>\n",
              "      <td>Dostoevsky</td>\n",
              "      <td>б каждый узнать весь секрет</td>\n",
              "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
              "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>В заключение скажу, что вами означенный долг ...</td>\n",
              "      <td>Dostoevsky</td>\n",
              "      <td>заключение сказать вы означенный долг рубль се...</td>\n",
              "      <td>[1, 0, 0, 0, 0, 0, 0, 1, 0, 0]</td>\n",
              "      <td>[0.77854435946222, 0.0, 0.0, 0.0, 0.0, 0.0, 0....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>— с достоинством и благородством заключил наш...</td>\n",
              "      <td>Dostoevsky</td>\n",
              "      <td>достоинство благородство заключить наш герой</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-830e1ee6-76c4-4e5b-a64e-9f4e13bcbf74')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-830e1ee6-76c4-4e5b-a64e-9f4e13bcbf74 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-830e1ee6-76c4-4e5b-a64e-9f4e13bcbf74');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "def get_text(author):\n",
        "    text = []\n",
        "    filenames = [x for x in os.listdir(f\"prose/{author}\") if x.endswith(\".txt\")]\n",
        "    for filename in filenames:\n",
        "        file = open(f\"prose/{author}/{filename}\", mode = \"r\", encoding=\"utf-8\")\n",
        "        text.append(file.read())\n",
        "        file.close()\n",
        "    return re.sub(r'\\s+', ' ' ,' '.join(text)) # убираем \\n\n",
        "\n",
        "\n",
        "def create_dataframe(authors):\n",
        "    column1 = []\n",
        "    column2 = []\n",
        "    for author in authors:\n",
        "        for sentence in authors[author][\"sentences\"]:\n",
        "            column1.append(sentence)\n",
        "            column2.append(author)\n",
        "    return pd.DataFrame({'sentence' : column1, 'author' : column2})\n",
        "\n",
        "\n",
        "def get_lemmas(sentence, stopwords):\n",
        "    lemmas = []\n",
        "    tokens = punct_tokenizer.tokenize(sentence)\n",
        "    # https://www.geeksforgeeks.org/string-punctuation-in-python/\n",
        "    punctuation = '\"' + \"!«!»?«?»«,»,)…—-!#$%&'()*+, -./:;<=>?@[\\]^_`{|}~–--\"\n",
        "    digits = \"0123456789\"\n",
        "    for token in tokens:\n",
        "        if token in punctuation or token[0] in digits:\n",
        "            continue\n",
        "        if token in stopwords:\n",
        "            continue\n",
        "        lemmas.append(morph_analyzer.parse(token)[0].normal_form)\n",
        "    return \" \".join(lemmas)\n",
        "\n",
        "\n",
        "def vectorize(data, vectorizer_class, column_index, column_name, max_features = 10):\n",
        "    vectorizer = vectorizer_class(max_features = max_features)\n",
        "    vectorizer.fit(data[\"lemmas\"])\n",
        "\n",
        "    result = vectorizer.transform(data[\"lemmas\"])\n",
        "    dense = np.asarray(result.todense())\n",
        "\n",
        "    values = [dense[i] for i in range(dense.shape[0])]\n",
        "    data.insert(column_index, column_name, values)\n",
        "\n",
        "    print(f'Самые популярные токены {column_name}: {vectorizer.get_feature_names()}')\n",
        "\n",
        "\n",
        "authors = {\"Dostoevsky\": {\"len\": 5000, \"sentences\": []},\n",
        "           \"Gogol\": {\"len\": 15000, \"sentences\": []},\n",
        "           \"Gorky\": {\"len\": 8000, \"sentences\": []},\n",
        "           \"Turgenev\": {\"len\": 11000, \"sentences\": []},\n",
        "           \"Tolstoy\": {\"len\": 20000, \"sentences\": []},\n",
        "           \"Pushkin\": {\"len\": 3000, \"sentences\": []}}\n",
        "\n",
        "\n",
        "# divide each text into sentences such that we will have two columns: sentence and target author, each row will contain one sentence and one target\n",
        "for author in authors:    \n",
        "    text = get_text(author)\n",
        "    expression = r\"[.|!|?|…]\" # https://qna.habr.com/q/393616\n",
        "    authors[author][\"sentences\"] = re.split(expression, text)\n",
        "\n",
        "\n",
        "# drop sentences where N symbols in a sentence < 15\n",
        "for author in authors:\n",
        "    sentences = authors[author][\"sentences\"]\n",
        "    authors[author][\"sentences\"] = [x for x in sentences if len(x.strip()) >= 15]\n",
        "\n",
        "\n",
        "# fix random state and randomly choose sentences in the folowing proportion \"5k : 15k : 8k : 11k : 20k : 3k\" for the authors respectively\n",
        "random.seed(83267)\n",
        "for author in authors:\n",
        "    authors[author][\"sentences\"] = random.sample(authors[author][\"sentences\"], authors[author][\"len\"])\n",
        " \n",
        "\n",
        "# tokenize, remove all stop words (nltk.corpus.stopwords), punctuation (string.punctuation) and numbers\n",
        "# convert to lower case and apply either stemming or lemmatization of the words (on your choice)\n",
        "nltk.download('stopwords')\n",
        "data = create_dataframe(authors)\n",
        "punct_tokenizer = nltk.WordPunctTokenizer()\n",
        "morph_analyzer = pymorphy2.MorphAnalyzer()\n",
        "stopwords = nltk.corpus.stopwords.words('russian')\n",
        "\n",
        "lemmas = []\n",
        "data = create_dataframe(authors)\n",
        "for sentence in data[\"sentence\"]:\n",
        "    lemmas.append(get_lemmas(sentence.lower(), stopwords))\n",
        "data.insert(2, 'lemmas', lemmas)\n",
        "\n",
        "# vectorize words using both bag of words and tf-idf (use sklearn)\n",
        "vectorize(data, CountVectorizer, 3, \"CountVectorizer\", max_features = 10)\n",
        "vectorize(data, TfidfVectorizer, 4, \"TfidfVectorizer\", max_features = 10)\n",
        "data.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) Самые популярные токены для двух vectorizer вычислились одинаково (я сначал подумал, что у меня ошибка в коде). Ими оказались слова ['говорить', 'знать', 'который', 'мочь', 'один', 'рука', 'свой', 'сказать', 'человек', 'это']. Выглядит логично.\n",
        "\n",
        "2) Что означают числа для CountVectorizer? Объясню на примере: <br>\n",
        "Леммы = \"посетить мой душа час говорить твёрдый вера...\" <br>\n",
        "Числа = [1, 0, 0, 0, 0, 0, 1, 0, 0, 0] <br>\n",
        "Популярыне токены = ['говорить', 'знать', 'который' ...]\n",
        "\n",
        "В массиве чисел на первом же месте стоит единица. Это означает, что токен стоящий на первом месте (\"говорить\") встречается в преложении один раз.\n",
        "Если бы вместо единица в массиве на первом месте стояло число 3 - это означало бы, что токен \"говорить\" встречается три раза. Думаю, логика понятна :)\n",
        "\n",
        "3) Что означают числа для TfidfVectorizer? <br>\n",
        "TF IDF VECTORIZER -> **term frequency / inverse document frequency** VECTORIZER <br>\n",
        "После расшифровки названия, становится понятно, что речь идет о частоте встречаемого слова в тексте. То есть чем чаще встречается токен, чем ближе число к единице. Расположение числа в массиве означает тоже самое, что и у CountVectorizer.\n",
        "\n",
        "Пользовался вот этим для TfidfVectorizer <br>\n",
        "https://ru.wikipedia.org/wiki/TF-IDF"
      ],
      "metadata": {
        "id": "axDaDY_kgKdu"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuTi3rvnDvZL"
      },
      "source": [
        "###  Binary classification\n",
        "\n",
        "#### 8. [2 point] Train model using Logistic Regression (your own) and SVC (SVM can be taken from sklearn) \n",
        "\n",
        "* choose *two* authors from the dataset that you have formed in the previous task\n",
        "* check the balance of the classes\n",
        "* divide the data into train and test samples with 0.7 split rate (don't forget to fix the random state)\n",
        "* using GridSearchCV - find the best parameters for the models (by F1 score) and use it in the next tasks\n",
        "* make several plots to address the dependence between F1 score and parameters\n",
        "* plot confusion matrix for train and test samples\n",
        "* compute some relevant metrics for test sample (useful to check the seminars 5 and 6, use sklearn) \n",
        "* make conclusions about the performance of your models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XZUP1HqFDvZL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "814028db-ddc9-4879-d190-ea99f0715d69"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=5, estimator=SVC(),\n",
              "             param_grid={'C': [0.01, 0.1, 1],\n",
              "                         'kernel': ['linear', 'poly', 'rbf']},\n",
              "             scoring='f1')"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "\n",
        "# choose two authors from the dataset that you have formed in the previous task\n",
        "limit = 1500\n",
        "data1 = data[(data[\"author\"] == \"Gogol\")][:limit]\n",
        "data2 = data[(data[\"author\"] == \"Gorky\")][:limit]\n",
        "\n",
        "# check the balance of the classes\n",
        "data1 = data1[:min(len(data1), len(data2))] # на случай, если убрать limit\n",
        "data2 = data2[:min(len(data1), len(data2))]\n",
        "data_local = data1.append(data2)\n",
        "\n",
        "# divide the data into train and test samples with 0.7 split rate (don't forget to fix the random state)\n",
        "x = np.vstack(data_local[\"TfidfVectorizer\"])\n",
        "y = np.where(data_local[\"author\"] == \"Gorky\", -1, 1)\n",
        "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 83267)\n",
        "\n",
        "logit = Logit(0, 0, learning_rate = 0.1)\n",
        "logit_fit = logit.fit(X_train, y_train)\n",
        "logit_prediction_1 = logit_fit.predict(X_train)\n",
        "logit_prediction_2 = logit_fit.predict(X_test)\n",
        "\n",
        "svc = SVC(kernel = \"linear\", C = 1)\n",
        "svc_fit = svc.fit(X_train, y_train)\n",
        "svc_prediction_1 = svc_fit.predict(X_train)\n",
        "svc_prediction_2 = svc_fit.predict(X_test)\n",
        "\n",
        "# using GridSearchCV - find the best parameters for the models (by F1 score) and use it in the next tasks\n",
        "# https://stats.stackexchange.com/questions/437072/use-f1-score-in-gridsearchcv\n",
        "grid_logit = {\"learning_rate\": [0.001, 0.01, 0.1], \"tolerance\": [1e-9, 1e-8, 1e-7]}\n",
        "searcher_logit = GridSearchCV(Logit(), param_grid = grid_logit, scoring = \"f1\", cv = 5)\n",
        "searcher_logit.fit(X_train, y_train)\n",
        "\n",
        "grid_logit_2 = {\"beta\": [0.1, 1, 2], \"gamma\": [0.1, 1, 2]}\n",
        "searcher_logit_2 = GridSearchCV(Logit(), param_grid = grid_logit_2, scoring = \"f1\", cv = 5)\n",
        "searcher_logit_2.fit(X_train, y_train)\n",
        "\n",
        "grid_svc = {\"kernel\": [\"linear\", \"poly\", \"rbf\"], \"C\": [0.01, 0.1, 1]}\n",
        "searcher_svc = GridSearchCV(SVC(), param_grid = grid_svc, scoring = \"f1\", cv = 5)\n",
        "searcher_svc.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def graphic_3d(x_array, y_array, z_array, x_best, y_best, title, x_label, y_label,\n",
        "               x_ticks = None, x_ticks_labels = None,\n",
        "               y_ticks = None, y_ticks_labels = None):\n",
        "    x = [[x_array[0], x_array[0], x_array[0]],\n",
        "         [x_array[1], x_array[1], x_array[1]],\n",
        "         [x_array[2], x_array[2], x_array[2]]]\n",
        "    y = [y_array, y_array, y_array]\n",
        "    z = np.reshape(z_array, (len(x_array), len(y_array)))\n",
        "\n",
        "    fig = plt.figure(figsize = (16, 8))\n",
        "    ax = plt.axes(projection = \"3d\")\n",
        "    ax.scatter3D(x, y, z, alpha = 0.2)\n",
        "    ax.plot_surface(x, y, z, alpha = 0.2)\n",
        "    ax.scatter3D(x_best, y_best, np.max(z), color = \"purple\")\n",
        "\n",
        "    plt.title(title)\n",
        "    plt.xlabel(x_label)\n",
        "    plt.ylabel(y_label)\n",
        "\n",
        "    if x_ticks is not None:\n",
        "        ax.set_xticks(x_ticks)\n",
        "    if x_ticks_labels is not None:\n",
        "        ax.set_xticklabels(x_ticks_labels)\n",
        "    if y_ticks is not None:\n",
        "        ax.set_yticks(y_ticks)\n",
        "    if y_ticks_labels is not None:\n",
        "        ax.set_yticklabels(y_ticks_labels)\n",
        "    \n",
        "    plt.show()\n",
        "\n",
        "\n",
        "best_params = {\"learning_rate\": searcher_logit.best_params_[\"learning_rate\"],\n",
        "               \"tolerance\": searcher_logit.best_params_[\"tolerance\"],\n",
        "               \"beta\": searcher_logit_2.best_params_[\"beta\"],\n",
        "               \"gamma\": searcher_logit_2.best_params_[\"gamma\"],\n",
        "               \"kernel\": searcher_svc.best_params_[\"kernel\"],\n",
        "               \"C\": searcher_svc.best_params_[\"C\"]}\n",
        "print(f\"Best params = {best_params}\")\n",
        "\n",
        "# make several plots to address the dependence between F1 score and parameters\n",
        "graphic_3d(grid_logit[\"learning_rate\"], grid_logit[\"tolerance\"], searcher_logit.cv_results_[\"mean_test_score\"],\n",
        "           best_params[\"learning_rate\"], best_params[\"tolerance\"],\n",
        "           title = \"Logit\", x_label = \"Learning rate\", y_label = \"Tolerance\")\n",
        "\n",
        "graphic_3d(grid_logit_2[\"beta\"], grid_logit_2[\"gamma\"], searcher_logit_2.cv_results_[\"mean_test_score\"],\n",
        "           best_params[\"beta\"], best_params[\"gamma\"],\n",
        "           title = \"Logit\", x_label = \"Beta\", y_label = \"Gamma\",\n",
        "           x_ticks = grid_logit_2[\"beta\"], y_ticks = grid_logit_2[\"gamma\"])\n",
        "\n",
        "graphic_3d(grid_svc[\"C\"], [0, 1, 2], searcher_svc.cv_results_[\"mean_test_score\"],\n",
        "           best_params[\"C\"], 2,\n",
        "           title = \"SVC\", x_label = \"C\", y_label = \"Kernel\",\n",
        "           y_ticks = [0, 1, 2], y_ticks_labels = [\"linear\", \"poly\", \"rbf\"])"
      ],
      "metadata": {
        "id": "rAjtnf_P5Eqx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) Logit (Learning rate, tolerance) <br>\n",
        "Лучшие значения: 'learning_rate': 0.1, 'tolerance': 1e-09 <br>\n",
        "Сразу заметим, что tolerance вообще не влияет на f1 score. При learning rate = 0.001 f1 score резко падает, при значениях 0.01 и 0.1 показывает неплохой уровень f1 score. Максимум находится на уровне learning_rate = 0.1 с любым tolerance.\n",
        "\n",
        "---\n",
        "\n",
        "2) Logit (Beta, Gamma) <br>\n",
        "Лучшие значения: 'beta': 2, 'gamma': 1 <br>\n",
        "График получился интересным. Экстремум находится в точке с гамма = 1 и бета = 2. При этом при изменении гаммы f1 score сильно падает. Но при бета равным 0.1 или 1 и гаммма равным 0.1 f1 score показывает значения лучше среднего по остальным точкам.\n",
        "\n",
        "---\n",
        "\n",
        "3) SVC (Kernel, C) <br>\n",
        "Лучшие значения: 'kernel': 'linear', 'C': 0.01 <br>\n",
        "Сразу заметим, что poly показывает результаты хуже, чем rbf и linear. При этом linear и rbf показывают практически идентиные результаты. При уменьшении параматера C их f1 score растет. Наилучший результат достигается при linear ядре и C = 0.01."
      ],
      "metadata": {
        "id": "YS-70su9gWBQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, confusion_matrix\n",
        "\n",
        "\n",
        "def print_metrics(train_or_test, prediction, model, dataset):\n",
        "    print(f\"{model} {dataset}\")\n",
        "    print(f\"Accuracy: {accuracy_score(train_or_test, prediction)}\")\n",
        "    print(f\"Roc Auc: {roc_auc_score(train_or_test, prediction)}\")\n",
        "    print(f\"F1: {f1_score(train_or_test, prediction)}\")\n",
        "    print()\n",
        "\n",
        "\n",
        "def graphic(axis, train_or_test, prediction, model, dataset):\n",
        "    names = ['Gogol','Gorky']\n",
        "    sns.heatmap(data=confusion_matrix(train_or_test, prediction),\n",
        "                annot = True, fmt = \"d\",cbar = False,\n",
        "                xticklabels = names, yticklabels = names, ax = axis)\n",
        "    axis.set_title(f\"{model} {dataset}\")\n",
        "\n",
        "\n",
        "# plot confusion matrix for train and test samples\n",
        "figures, axis = plt.subplots(1, 2, figsize = (24, 6))\n",
        "graphic(axis[0], y_train, logit_prediction_1, \"Logit\", \"train\")\n",
        "graphic(axis[1], y_train, svc_prediction_1, \"SVC\", \"train\")\n",
        "plt.show()\n",
        "\n",
        "figures, axis = plt.subplots(1, 2, figsize = (24, 6))\n",
        "graphic(axis[0], y_test, logit_prediction_2, \"Logit\", \"test\")\n",
        "graphic(axis[1], y_test, svc_prediction_2, \"SVC\", \"test\")\n",
        "plt.show()\n",
        "\n",
        "# compute some relevant metrics for test sample (useful to check the seminars 5 and 6, use sklearn)\n",
        "print_metrics(y_test, logit_prediction_2, \"Logit\", \"test\")\n",
        "print_metrics(y_test, svc_prediction_2, \"SVC\", \"test\")"
      ],
      "metadata": {
        "id": "3VrpoJ7kkmfE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TP (TN) = true positive (negative) <br>\n",
        "FP (FN) = false positive (negative) <br>\n",
        "\n",
        "1) Сразу заметим, что результаты моей модели (logit) очень близки к результатам SVC модели - каждое число отличается буквально на несколько процентов. Что это означает? Во-первых, это успех :) Во-вторых, нет смысла рассматривать отдельно матрицы, так они идентичны, поэтому буду рассматривать только матрицы logit модели.\n",
        "\n",
        "2) В обоих случаях TP + TN > FP + FN, то есть accuracy дейстильно больше 0.5 - уже неплохо. В частности TP > FN и TN > FP, то есть модель ошибается грубоговоря \"равномерно\" в обоих случаях, не перевешиваясь в одну крайность - тоже неплохо.\n",
        "\n",
        "3) Также можно выделить достаточно низкое кол-во FP, то есть если модель выдает positive результат, то у него достаточно большая вероятность быть TP, что можно использовать."
      ],
      "metadata": {
        "id": "vVLCAh3bgMRR"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5G1kt0qbDvZL"
      },
      "source": [
        "#### 9. [1 point] Analysing ROC AUC\n",
        "\n",
        "It is possible to control the proportion of statistical errors of different types using different thresholds for choosing a class. Plot ROC curves for Logistic Regression and SVC, show the threshold on ROC curve plots. Choose such a threshold that your models have no more than 30% of false positive errors rate. Pay attention to `thresholds` parameter in sklearn roc_curve "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZ2GZ8-uDvZL"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_curve, RocCurveDisplay\n",
        "\n",
        "\n",
        "## https://stackoverflow.com/questions/25009284/how-to-plot-roc-curve-in-python\n",
        "def grapchic_roc(model, predict_proba = True):\n",
        "    if predict_proba:\n",
        "        probs = model.predict_proba(X_test)\n",
        "        fpr, tpr, threshold = roc_curve(y_test, probs[:,1])\n",
        "    else:\n",
        "        decision = model.decision_function(X_test)\n",
        "        fpr, tpr, threshold = roc_curve(y_test, decision)\n",
        "\n",
        "    curve = RocCurveDisplay.from_estimator(model, X_test, y_test)\n",
        "    curve.figure_.suptitle(type(model).__name__)\n",
        "    plt.scatter([fpr], [tpr])\n",
        "    plt.axvline(x = 0.3, color = \"purple\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "logit = LogisticRegression(C = 1, max_iter=1000)\n",
        "logit.fit(X_train, y_train)\n",
        "grapchic_roc(logit)\n",
        "\n",
        "svc = SVC(kernel = \"linear\", C = 1)\n",
        "svc.fit(X_train, y_train)\n",
        "grapchic_roc(svc, False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "При FPR = 0.3, TPR = 0.4 - не очень хороший результат, но как есть на этих данных."
      ],
      "metadata": {
        "id": "gpA4i6Ia9hZ_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-qubaK4DvZM"
      },
      "source": [
        "### Multiclass logit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJQone-LDvZM"
      },
      "source": [
        "#### 10. [1 point] Take the One-VS-One classifier (use sklearn) and apply to Logit model (one you've made in the 4th task) in order to get multiclass linear classifier\n",
        "\n",
        "<a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsOneClassifier.html\">OneVsOneClassifier</a>\n",
        "\n",
        "* use the data you got at the previous step for 6 authors\n",
        "* divide the data into train and test samples with 0.7 split rate\n",
        "* using GridSearchCV - find the best parameters for the models (by F1 score)\n",
        "* plot confusion matrix for train and test samples\n",
        "* compute all possible and relevant metrics for test sample (use sklearn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M4lR8qJ7DvZM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "66e0782f-fb27-40c1-e036-efe51a6f5949"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# divide the data into train and test samples with 0.7 split rate\\ndata_local = data.sample(200, random_state = 83267)\\nx = np.vstack(data_local[\"TfidfVectorizer\"])\\ny = data_local[\"author\"]\\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 83267, stratify = y)\\n\\n# using GridSearchCV - find the best parameters for the models (by F1 score)\\ngrid_1 = {\"estimator__learning_rate\": [0.001, 0.01, 0.1], \"estimator__tolerance\": [1e-9, 1e-8, 1e-7]}\\nsearcher_1 = GridSearchCV(OneVsOneClassifier(Logit()), param_grid = grid_1, scoring = \"f1\", cv = 5)\\nsearcher_1.fit(X_train, y_train)\\n\\ngrid_2 = {\"estimator__beta\": [0.1, 1, 2], \"estimator__gamma\": [0.1, 1, 2]}\\nsearcher_2 = GridSearchCV(OneVsOneClassifier(Logit()), param_grid = grid_2, scoring = \"f1\", cv = 5)\\nsearcher_2.fit(X_train, y_train)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "from collections import Counter\n",
        "from sklearn.multiclass import OneVsOneClassifier\n",
        "from sklearn.metrics import make_scorer, f1_score\n",
        "\n",
        "'''\n",
        "# divide the data into train and test samples with 0.7 split rate\n",
        "data_local = data.sample(200, random_state = 83267)\n",
        "x = np.vstack(data_local[\"TfidfVectorizer\"])\n",
        "y = data_local[\"author\"]\n",
        "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 83267, stratify = y)\n",
        "\n",
        "# using GridSearchCV - find the best parameters for the models (by F1 score)\n",
        "grid_1 = {\"estimator__learning_rate\": [0.001, 0.01, 0.1], \"estimator__tolerance\": [1e-9, 1e-8, 1e-7]}\n",
        "searcher_1 = GridSearchCV(OneVsOneClassifier(Logit()), param_grid = grid_1, scoring = \"f1\", cv = 5)\n",
        "searcher_1.fit(X_train, y_train)\n",
        "\n",
        "grid_2 = {\"estimator__beta\": [0.1, 1, 2], \"estimator__gamma\": [0.1, 1, 2]}\n",
        "searcher_2 = GridSearchCV(OneVsOneClassifier(Logit()), param_grid = grid_2, scoring = \"f1\", cv = 5)\n",
        "searcher_2.fit(X_train, y_train)\n",
        "'''\n",
        "\n",
        "# ПО НЕ ПОНЯТНЫМ МНЕ ПРИЧИНАМ, ОНО НЕ ХОЧЕТ РАБОТАТЬ"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, confusion_matrix\n",
        "\n",
        "'''\n",
        "def print_metrics(train_or_test, prediction, dataset):\n",
        "    print(f\"{dataset}\")\n",
        "    print(f\"Accuracy: {accuracy_score(train_or_test, prediction)}\")\n",
        "    print(f\"Roc Auc: {roc_auc_score(train_or_test, prediction)}\")\n",
        "    print(f\"F1: {f1_score(train_or_test, prediction)}\")\n",
        "    print()\n",
        "\n",
        "\n",
        "# xticklabels = names, yticklabels = names, \n",
        "def graphic(axis, train_or_test, prediction, dataset):\n",
        "    sns.heatmap(data=confusion_matrix(train_or_test, prediction),\n",
        "                annot = True, fmt = \"d\",cbar = False, ax = axis)\n",
        "    axis.set_title(f\"{dataset}\")\n",
        "\n",
        "\n",
        "best_params = {\"learning_rate\": searcher_1.best_params_[\"estimator__learning_rate\"],\n",
        "               \"tolerance\": searcher_1.best_params_[\"estimator__tolerance\"],\n",
        "               \"beta\": searcher_2.best_params_[\"estimator__beta\"],\n",
        "               \"gamma\": searcher_2.best_params_[\"estimator__gamma\"]}\n",
        "print(f\"Best params = {best_params}\")\n",
        "\n",
        "logit = Logit(learning_rate = 0.001, tolerance = 1e-09, beta = 0.1, gamma = 0.1)\n",
        "logit_fit = logit.fit(X_train, y_train)\n",
        "logit_prediction_1 = logit_fit.predict(X_train)\n",
        "logit_prediction_2 = logit_fit.predict(X_test)\n",
        "\n",
        "# plot confusion matrix for train and test samples\n",
        "figures, axis = plt.subplots(1, 2, figsize = (24, 6))\n",
        "graphic(axis[0], y_train, logit_prediction_1, \"Train\")\n",
        "graphic(axis[1], y_test, logit_prediction_2, \"Test\")\n",
        "plt.show()\n",
        "\n",
        "# compute all possible and relevant metrics for test sample (use sklearn)\n",
        "print_metrics(y_test, logit_prediction_2, \"Test\")\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "4A7P7wLMKLxP",
        "outputId": "b76cb1af-15f7-4cd1-9505-c119c2d8ce4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ndef print_metrics(train_or_test, prediction, dataset):\\n    print(f\"{dataset}\")\\n    print(f\"Accuracy: {accuracy_score(train_or_test, prediction)}\")\\n    print(f\"Roc Auc: {roc_auc_score(train_or_test, prediction)}\")\\n    print(f\"F1: {f1_score(train_or_test, prediction)}\")\\n    print()\\n\\n\\n# xticklabels = names, yticklabels = names, \\ndef graphic(axis, train_or_test, prediction, dataset):\\n    sns.heatmap(data=confusion_matrix(train_or_test, prediction),\\n                annot = True, fmt = \"d\",cbar = False, ax = axis)\\n    axis.set_title(f\"{dataset}\")\\n\\n\\nbest_params = {\"learning_rate\": searcher_1.best_params_[\"estimator__learning_rate\"],\\n               \"tolerance\": searcher_1.best_params_[\"estimator__tolerance\"],\\n               \"beta\": searcher_2.best_params_[\"estimator__beta\"],\\n               \"gamma\": searcher_2.best_params_[\"estimator__gamma\"]}\\nprint(f\"Best params = {best_params}\")\\n\\nlogit = Logit(learning_rate = 0.001, tolerance = 1e-09, beta = 0.1, gamma = 0.1)\\nlogit_fit = logit.fit(X_train, y_train)\\nlogit_prediction_1 = logit_fit.predict(X_train)\\nlogit_prediction_2 = logit_fit.predict(X_test)\\n\\n# plot confusion matrix for train and test samples\\nfigures, axis = plt.subplots(1, 2, figsize = (24, 6))\\ngraphic(axis[0], y_train, logit_prediction_1, \"Train\")\\ngraphic(axis[1], y_test, logit_prediction_2, \"Test\")\\nplt.show()\\n\\n# compute all possible and relevant metrics for test sample (use sklearn)\\nprint_metrics(y_test, logit_prediction_2, \"Test\")\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}